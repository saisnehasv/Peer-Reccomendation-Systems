{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8568ba03-2cca-46ac-bf0e-e30319f87780",
   "metadata": {},
   "source": [
    "# Peer Recommendation System\n",
    "\n",
    "Course: SI 670: Applied Machine Learning\n",
    "\n",
    "Name : Yuganshi Agrawal  \n",
    "uniqname: yuganshi\n",
    "\n",
    "Name : Sai Sneha Siddapura Venkataramappa  \n",
    "uniqname: saisneha\n",
    "\n",
    "### Notebook 03: Label Generation\n",
    "\n",
    "This notebook creates complementarity labels for student pairs.\n",
    "\n",
    "**Inputs:**\n",
    "- `data/features/feature_matrix_scaled.pkl`\n",
    "- `data/features/features_proc_unique.pkl`\n",
    "- `data/processed/student_info_clean.pkl`\n",
    "- `data/processed/student_split.pkl`\n",
    "\n",
    "**Outputs:**\n",
    "- `data/processed/student_pairs_train.pkl`\n",
    "- `data/processed/student_pairs_test.pkl`\n",
    "- `data/processed/complementarity_definition.json`\n",
    "- `results/analysis/label_distribution.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5771a44-0d6c-45f6-b78e-789854a4cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "if 'CUDA_VISIBLE_DEVICES' not in os.environ:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a9667e-c969-45e2-9f3d-5d2c27e60b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Configuration:\n",
      "  CPUs available: 32\n",
      "  Device: cpu (CUDA initialization failed: CUDA call failed lazily at initialization with error: device >= 0 && device < nu)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "\n",
    "# Cell 2: Hardware detection with robust error handling\n",
    "N_CPU = mp.cpu_count()\n",
    "DEVICE = 'cpu'  # Default to CPU\n",
    "N_GPU = 0\n",
    "\n",
    "print(\"Hardware Configuration:\")\n",
    "print(f\"  CPUs available: {N_CPU}\")\n",
    "\n",
    "# Try to detect and initialize CUDA\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        N_GPU = torch.cuda.device_count()\n",
    "        if N_GPU > 0:\n",
    "            # Test that we can actually access GPU 0\n",
    "            torch.cuda.set_device(0)\n",
    "            test_tensor = torch.zeros(1).cuda()\n",
    "            del test_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            DEVICE = 'cuda'\n",
    "            print(f\"  Device: cuda\")\n",
    "            print(f\"  GPUs accessible: {N_GPU}\")\n",
    "            \n",
    "            for i in range(N_GPU):\n",
    "                try:\n",
    "                    name = torch.cuda.get_device_name(i)\n",
    "                    mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "                    print(f\"    GPU {i}: {name} ({mem:.1f} GB)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    GPU {i}: Error accessing - {str(e)[:50]}\")\n",
    "        else:\n",
    "            print(f\"  Device: cpu (CUDA available but no GPUs detected)\")\n",
    "    else:\n",
    "        print(f\"  Device: cpu (CUDA not available)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  Device: cpu (CUDA initialization failed: {str(e)[:80]})\")\n",
    "    DEVICE = 'cpu'\n",
    "    N_GPU = 0\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4124253-0115-4f17-8215-7e99a3d93a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Configuration:\n",
      "  Mode: CPU multiprocessing\n",
      "  Workers: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if DEVICE == 'cuda':\n",
    "    N_WORKERS = min(4, max(1, N_CPU // 4))  # Fewer workers for GPU mode\n",
    "else:\n",
    "    N_WORKERS = max(1, min(N_CPU - 1, 16))  # More workers for CPU mode, cap at 16\n",
    "\n",
    "print(f\"Parallel Configuration:\")\n",
    "print(f\"  Mode: {'GPU-accelerated' if DEVICE == 'cuda' else 'CPU multiprocessing'}\")\n",
    "print(f\"  Workers: {N_WORKERS}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adbc56-1c63-4831-b8c6-07de157a14b4",
   "metadata": {},
   "source": [
    "## Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "454e208b-1fa1-4ef8-b874-583d0f9e59f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure:\n",
      "  Processed data: ../670-Project/data/processed\n",
      "  Features: ../670-Project/data/features\n",
      "  Analysis output: ../670-Project/results/analysis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path('../670-Project')\n",
    "DATA_PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
    "DATA_FEATURES_DIR = BASE_DIR / 'data' / 'features'\n",
    "RESULTS_ANALYSIS_DIR = BASE_DIR / 'results' / 'analysis'\n",
    "\n",
    "RESULTS_ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure:\")\n",
    "print(f\"  Processed data: {DATA_PROCESSED_DIR}\")\n",
    "print(f\"  Features: {DATA_FEATURES_DIR}\")\n",
    "print(f\"  Analysis output: {RESULTS_ANALYSIS_DIR}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72a1f6-6d14-4b29-9856-50427b8ed67f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0be318f-0cd3-4321-8b2a-47b91ead1b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Max pairs per module: 30,000\n",
      "  Complementarity threshold: 0.8 percentile\n",
      "  Random seed: 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_PAIRS_PER_MODULE = 30000\n",
    "COMPLEMENTARITY_THRESHOLD_PERCENTILE = 0.80\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Max pairs per module: {MAX_PAIRS_PER_MODULE:,}\")\n",
    "print(f\"  Complementarity threshold: {COMPLEMENTARITY_THRESHOLD_PERCENTILE} percentile\")\n",
    "print(f\"  Random seed: {RNG_SEED}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c938e7-0be1-47b1-ad30-7f2c037ebf20",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load features and metadata from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ca4709-dd01-4d52-a397-68c359a13062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Loaded data:\n",
      "  Feature matrix: (28785, 100)\n",
      "  Unique students: 28,785\n",
      "  Assessment profiles: (23351, 188)\n",
      "  Training students: 25,907\n",
      "  Holdout students: 2,878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "print()\n",
    "\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "feature_matrix_scaled = load_pickle(DATA_FEATURES_DIR / 'feature_matrix_scaled.pkl')\n",
    "features_proc_unique = load_pickle(DATA_FEATURES_DIR / 'features_proc_unique.pkl')\n",
    "student_info = load_pickle(DATA_PROCESSED_DIR / 'student_info_clean.pkl')\n",
    "student_split = load_pickle(DATA_PROCESSED_DIR / 'student_split.pkl')\n",
    "assessment_features = load_pickle(DATA_FEATURES_DIR / 'assessment_features.pkl')\n",
    "\n",
    "assess_pivot_z = assessment_features['assess_pivot_z']\n",
    "\n",
    "train_students = student_split['train_students']\n",
    "holdout_students = student_split['holdout_students']\n",
    "\n",
    "with open(DATA_FEATURES_DIR / 'feature_metadata.json', 'r') as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "print(\"Loaded data:\")\n",
    "print(f\"  Feature matrix: {feature_matrix_scaled.shape}\")\n",
    "print(f\"  Unique students: {len(features_proc_unique):,}\")\n",
    "print(f\"  Assessment profiles: {assess_pivot_z.shape}\")\n",
    "print(f\"  Training students: {len(train_students):,}\")\n",
    "print(f\"  Holdout students: {len(holdout_students):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debe450-8f25-4215-be26-e24740b2d094",
   "metadata": {},
   "source": [
    "## Complementarity Definition\n",
    "\n",
    "Define complementarity as a combination of:\n",
    "1. **Skill complementarity**: Moderate distance in assessment profiles\n",
    "2. **Engagement complementarity**: Different activity patterns\n",
    "\n",
    "Students are complementary when they have:\n",
    "- Different skill strengths (can help each other)\n",
    "- Different engagement patterns (can balance each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf4b9c55-d391-4852-8e5e-c3becc74ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing assessment embeddings for complementarity computation...\n",
      "\n",
      "Assessment embedding columns: 48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing assessment embeddings for complementarity computation...\")\n",
    "print()\n",
    "\n",
    "assess_cols = [c for c in features_proc_unique.columns if c.startswith('assess_emb_')]\n",
    "print(f\"Assessment embedding columns: {len(assess_cols)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482ae57f-1e86-4f20-90c9-657fb6276d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_complementarity_batch_gpu(pairs_batch, features_dict, assess_lookup, assess_tensor):\n",
    "    batch_size = len(pairs_batch)\n",
    "    comp_scores = np.zeros(batch_size)\n",
    "    skill_comps = np.zeros(batch_size)\n",
    "    engage_comps = np.zeros(batch_size)\n",
    "    \n",
    "    valid_mask = []\n",
    "    indices_a = []\n",
    "    indices_b = []\n",
    "    \n",
    "    for i, (sid_a, sid_b) in enumerate(pairs_batch):\n",
    "        if sid_a in assess_lookup and sid_b in assess_lookup:\n",
    "            indices_a.append(assess_lookup[sid_a])\n",
    "            indices_b.append(assess_lookup[sid_b])\n",
    "            valid_mask.append(i)\n",
    "    \n",
    "    if len(valid_mask) > 0:\n",
    "        idx_a = torch.LongTensor(indices_a).cuda()\n",
    "        idx_b = torch.LongTensor(indices_b).cuda()\n",
    "        \n",
    "        vecs_a = assess_tensor[idx_a]\n",
    "        vecs_b = assess_tensor[idx_b]\n",
    "        \n",
    "        norm_a = F.normalize(vecs_a, dim=1)\n",
    "        norm_b = F.normalize(vecs_b, dim=1)\n",
    "        \n",
    "        correlations = (norm_a * norm_b).sum(dim=1)\n",
    "        \n",
    "        distances = torch.sqrt(((norm_a - norm_b) ** 2).sum(dim=1) + 1e-8)\n",
    "        distance_scores = torch.exp(-((distances - 0.7) ** 2) / 0.3)\n",
    "        \n",
    "        complement_patterns = torch.abs(norm_a - norm_b) * (norm_a + norm_b)\n",
    "        complement_patterns = complement_patterns.mean(dim=1)\n",
    "        \n",
    "        skill_comp_batch = (\n",
    "            0.40 * (1 - correlations) +\n",
    "            0.35 * distance_scores +\n",
    "            0.25 * complement_patterns\n",
    "        ).cpu().numpy()\n",
    "        \n",
    "        skill_comps[valid_mask] = np.clip(skill_comp_batch, 0, 1)\n",
    "    \n",
    "    for i, (sid_a, sid_b) in enumerate(pairs_batch):\n",
    "        if sid_a in features_dict and sid_b in features_dict:\n",
    "            feat_a = features_dict[sid_a]\n",
    "            feat_b = features_dict[sid_b]\n",
    "            \n",
    "            type_div_diff = abs(feat_a['type_diversity'] - feat_b['type_diversity'])\n",
    "            type_div_score = type_div_diff / (feat_a['type_diversity'] + feat_b['type_diversity'] + 1)\n",
    "            \n",
    "            entropy_diff = abs(feat_a['type_entropy'] - feat_b['type_entropy'])\n",
    "            entropy_score = entropy_diff / (feat_a['type_entropy'] + feat_b['type_entropy'] + 1)\n",
    "            \n",
    "            min_activity = min(feat_a['week_mean'], feat_b['week_mean'])\n",
    "            activity_score = min(min_activity / 10, 1.0)\n",
    "            \n",
    "            engage_comps[i] = 0.35 * type_div_score + 0.35 * entropy_score + 0.30 * activity_score\n",
    "    \n",
    "    comp_scores = 0.60 * skill_comps + 0.40 * engage_comps\n",
    "    \n",
    "    return comp_scores, skill_comps, engage_comps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaaa533f-d1b3-4d18-8870-6bfecb7dbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_complementarity_batch_cpu(pairs_batch, features_dict, assess_array, assess_lookup):\n",
    "    batch_size = len(pairs_batch)\n",
    "    comp_scores = np.zeros(batch_size)\n",
    "    skill_comps = np.zeros(batch_size)\n",
    "    engage_comps = np.zeros(batch_size)\n",
    "    \n",
    "    valid_mask = []\n",
    "    indices_a = []\n",
    "    indices_b = []\n",
    "    \n",
    "    for i, (sid_a, sid_b) in enumerate(pairs_batch):\n",
    "        if sid_a in assess_lookup and sid_b in assess_lookup:\n",
    "            indices_a.append(assess_lookup[sid_a])\n",
    "            indices_b.append(assess_lookup[sid_b])\n",
    "            valid_mask.append(i)\n",
    "    \n",
    "    if len(valid_mask) > 0:\n",
    "        vecs_a = assess_array[indices_a]\n",
    "        vecs_b = assess_array[indices_b]\n",
    "        \n",
    "        # Normalize vectors\n",
    "        norm_a = vecs_a / (np.linalg.norm(vecs_a, axis=1, keepdims=True) + 1e-8)\n",
    "        norm_b = vecs_b / (np.linalg.norm(vecs_b, axis=1, keepdims=True) + 1e-8)\n",
    "        \n",
    "        # Compute correlations\n",
    "        correlations = (norm_a * norm_b).sum(axis=1)\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = np.sqrt(((norm_a - norm_b) ** 2).sum(axis=1) + 1e-8)\n",
    "        distance_scores = np.exp(-((distances - 0.7) ** 2) / 0.3)\n",
    "        \n",
    "        # Compute complement patterns\n",
    "        complement_patterns = np.abs(norm_a - norm_b) * (norm_a + norm_b)\n",
    "        complement_patterns = complement_patterns.mean(axis=1)\n",
    "        \n",
    "        # Combine skill components\n",
    "        skill_comp_batch = (\n",
    "            0.40 * (1 - correlations) +\n",
    "            0.35 * distance_scores +\n",
    "            0.25 * complement_patterns\n",
    "        )\n",
    "        \n",
    "        skill_comps[valid_mask] = np.clip(skill_comp_batch, 0, 1)\n",
    "    \n",
    "    # Compute engagement complementarity\n",
    "    for i, (sid_a, sid_b) in enumerate(pairs_batch):\n",
    "        if sid_a in features_dict and sid_b in features_dict:\n",
    "            feat_a = features_dict[sid_a]\n",
    "            feat_b = features_dict[sid_b]\n",
    "            \n",
    "            type_div_diff = abs(feat_a['type_diversity'] - feat_b['type_diversity'])\n",
    "            type_div_score = type_div_diff / (feat_a['type_diversity'] + feat_b['type_diversity'] + 1)\n",
    "            \n",
    "            entropy_diff = abs(feat_a['type_entropy'] - feat_b['type_entropy'])\n",
    "            entropy_score = entropy_diff / (feat_a['type_entropy'] + feat_b['type_entropy'] + 1)\n",
    "            \n",
    "            min_activity = min(feat_a['week_mean'], feat_b['week_mean'])\n",
    "            activity_score = min(min_activity / 10, 1.0)\n",
    "            \n",
    "            engage_comps[i] = 0.35 * type_div_score + 0.35 * entropy_score + 0.30 * activity_score\n",
    "    \n",
    "    comp_scores = 0.60 * skill_comps + 0.40 * engage_comps\n",
    "    \n",
    "    return comp_scores, skill_comps, engage_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ffdfd37-15d7-4ea4-ab5b-5ace6648585c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complementarity computation functions defined\n",
      "  Mode: CPU with multiprocessing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_pair_chunk_cpu(args):\n",
    "    \"\"\"Worker function for multiprocessing on CPU.\"\"\"\n",
    "    pairs_chunk, features_dict, assess_array, assess_lookup = args\n",
    "    return compute_complementarity_batch_cpu(pairs_chunk, features_dict, assess_array, assess_lookup)\n",
    "\n",
    "\n",
    "print(\"Complementarity computation functions defined\")\n",
    "print(f\"  Mode: {'GPU' if DEVICE == 'cuda' else 'CPU with multiprocessing'}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a08ca5a8-e70e-495b-a859-063f38940cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature lookups...\n",
      "\n",
      "\n",
      "Assessment lookup: 23,351 students\n",
      "Features dict: 28,785 students\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing feature lookups...\")\n",
    "print()\n",
    "\n",
    "assess_index = assess_pivot_z.index.tolist()\n",
    "assess_lookup = {sid: idx for idx, sid in enumerate(assess_index)}\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    try:\n",
    "        assess_tensor = torch.FloatTensor(assess_pivot_z.values).cuda()\n",
    "        assess_array = None\n",
    "    except Exception as e:\n",
    "        N_WORKERS = max(1, min(N_CPU - 1, 16))\n",
    "        assess_tensor = None\n",
    "        assess_array = assess_pivot_z.values.astype(np.float32)\n",
    "else:\n",
    "    assess_tensor = None\n",
    "    assess_array = assess_pivot_z.values.astype(np.float32)\n",
    "\n",
    "features_dict = {}\n",
    "for _, row in features_proc_unique.iterrows():\n",
    "    features_dict[row['id_student']] = {\n",
    "        'type_diversity': row['type_diversity'],\n",
    "        'type_entropy': row['type_entropy'],\n",
    "        'week_mean': row['week_mean']\n",
    "    }\n",
    "\n",
    "print()\n",
    "print(f\"Assessment lookup: {len(assess_lookup):,} students\")\n",
    "print(f\"Features dict: {len(features_dict):,} students\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb61f486-d649-43d4-8f7d-6d5ebdf485cb",
   "metadata": {},
   "source": [
    "## Generate Student Pairs\n",
    "\n",
    "Generate pairs within each module-presentation group.\n",
    "Sample if exceeds maximum pairs per module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "677c93d2-6822-42ca-b8aa-3903b87dbb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating student pairs per module...\n",
      "\n",
      "Processing 22 module-presentation groups...\n",
      "  Batch size: 2000\n",
      "  Parallel workers: 16\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec513788b6049c68853112dbadd6055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating pairs:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 660,000 pairs\n",
      "  Across 22 module-presentation groups\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating student pairs per module...\")\n",
    "print()\n",
    "\n",
    "COMP_BATCH_SIZE = 5000 if DEVICE == 'cuda' else 2000\n",
    "\n",
    "pair_data = []\n",
    "grouped = features_proc_unique.groupby(['code_module', 'code_presentation'])\n",
    "\n",
    "print(f\"Processing {grouped.ngroups} module-presentation groups...\")\n",
    "print(f\"  Batch size: {COMP_BATCH_SIZE}\")\n",
    "if DEVICE == 'cpu':\n",
    "    print(f\"  Parallel workers: {N_WORKERS}\")\n",
    "print()\n",
    "\n",
    "for (mod, pres), group in tqdm(grouped, desc=\"Creating pairs\"):\n",
    "    group_dedup = group.drop_duplicates(subset=['id_student'], keep='first')\n",
    "    students = group_dedup['id_student'].unique().tolist()\n",
    "    \n",
    "    if len(students) < 2:\n",
    "        continue\n",
    "    \n",
    "    all_pairs = list(itertools.combinations(students, 2))\n",
    "    \n",
    "    if len(all_pairs) > MAX_PAIRS_PER_MODULE:\n",
    "        all_pairs = random.sample(all_pairs, MAX_PAIRS_PER_MODULE)\n",
    "    \n",
    "    if DEVICE == 'cuda':\n",
    "        # GPU processing - sequential batches\n",
    "        for batch_start in range(0, len(all_pairs), COMP_BATCH_SIZE):\n",
    "            batch_end = min(batch_start + COMP_BATCH_SIZE, len(all_pairs))\n",
    "            batch_pairs = all_pairs[batch_start:batch_end]\n",
    "            \n",
    "            comp_scores, skill_comps, engage_comps = compute_complementarity_batch_gpu(\n",
    "                batch_pairs, features_dict, assess_lookup, assess_tensor\n",
    "            )\n",
    "            \n",
    "            for idx, (sid_a, sid_b) in enumerate(batch_pairs):\n",
    "                pair_data.append({\n",
    "                    'id_i': sid_a,\n",
    "                    'id_j': sid_b,\n",
    "                    'code_module': mod,\n",
    "                    'code_presentation': pres,\n",
    "                    'comp_score': comp_scores[idx],\n",
    "                    'skill_comp': skill_comps[idx],\n",
    "                    'engage_comp': engage_comps[idx]\n",
    "                })\n",
    "    else:\n",
    "        # CPU multiprocessing - parallel batches\n",
    "        # Split pairs into chunks for parallel processing\n",
    "        chunks = []\n",
    "        for batch_start in range(0, len(all_pairs), COMP_BATCH_SIZE):\n",
    "            batch_end = min(batch_start + COMP_BATCH_SIZE, len(all_pairs))\n",
    "            batch_pairs = all_pairs[batch_start:batch_end]\n",
    "            chunks.append((batch_pairs, features_dict, assess_array, assess_lookup))\n",
    "        \n",
    "        # Process chunks in parallel\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(process_pair_chunk_cpu, chunks)\n",
    "        \n",
    "        # Aggregate results\n",
    "        chunk_idx = 0\n",
    "        for batch_start in range(0, len(all_pairs), COMP_BATCH_SIZE):\n",
    "            batch_end = min(batch_start + COMP_BATCH_SIZE, len(all_pairs))\n",
    "            batch_pairs = all_pairs[batch_start:batch_end]\n",
    "            \n",
    "            comp_scores, skill_comps, engage_comps = results[chunk_idx]\n",
    "            chunk_idx += 1\n",
    "            \n",
    "            for idx, (sid_a, sid_b) in enumerate(batch_pairs):\n",
    "                pair_data.append({\n",
    "                    'id_i': sid_a,\n",
    "                    'id_j': sid_b,\n",
    "                    'code_module': mod,\n",
    "                    'code_presentation': pres,\n",
    "                    'comp_score': comp_scores[idx],\n",
    "                    'skill_comp': skill_comps[idx],\n",
    "                    'engage_comp': engage_comps[idx]\n",
    "                })\n",
    "\n",
    "pairs_df = pd.DataFrame(pair_data)\n",
    "\n",
    "print()\n",
    "print(f\"Generated {len(pairs_df):,} pairs\")\n",
    "print(f\"  Across {grouped.ngroups} module-presentation groups\")\n",
    "print()\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2f5b6-05e3-44fd-a100-13e0ed56e61c",
   "metadata": {},
   "source": [
    "## Create Binary Labels\n",
    "\n",
    "Convert complementarity scores to binary labels using percentile threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4e4c6fa-acc4-48cc-85c9-98ba2eb8495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating binary labels...\n",
      "\n",
      "Complementarity threshold: 0.3978\n",
      "\n",
      "Complementarity score statistics:\n",
      "  Mean: 1.1179\n",
      "  Std: 717.8133\n",
      "  Min: -21677.0041\n",
      "  Max: 560557.2619\n",
      "  Median: 0.2512\n",
      "\n",
      "Label distribution:\n",
      "  Label 0: 528,000 (80.00%)\n",
      "  Label 1: 132,000 (20.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating binary labels...\")\n",
    "print()\n",
    "\n",
    "threshold = pairs_df['comp_score'].quantile(COMPLEMENTARITY_THRESHOLD_PERCENTILE)\n",
    "pairs_df['label'] = (pairs_df['comp_score'] >= threshold).astype(int)\n",
    "\n",
    "print(f\"Complementarity threshold: {threshold:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Complementarity score statistics:\")\n",
    "print(f\"  Mean: {pairs_df['comp_score'].mean():.4f}\")\n",
    "print(f\"  Std: {pairs_df['comp_score'].std():.4f}\")\n",
    "print(f\"  Min: {pairs_df['comp_score'].min():.4f}\")\n",
    "print(f\"  Max: {pairs_df['comp_score'].max():.4f}\")\n",
    "print(f\"  Median: {pairs_df['comp_score'].median():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "label_counts = pairs_df['label'].value_counts().sort_index()\n",
    "for label, count in label_counts.items():\n",
    "    pct = count / len(pairs_df) * 100\n",
    "    print(f\"  Label {label}: {count:,} ({pct:.2f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22368b4-a781-4034-a86c-9f1a2bb035f9",
   "metadata": {},
   "source": [
    "## Split Pairs into Train/Test\n",
    "\n",
    "Split based on student holdout:\n",
    "- If either student in pair is in holdout set, pair goes to test\n",
    "- Otherwise pair goes to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64ee6074-b950-4eee-a1c6-60a77bd03f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/test split...\n",
      "\n",
      "Train pairs: 535,953\n",
      "  Positive: 107,910 (20.13%)\n",
      "  Negative: 428,043 (79.87%)\n",
      "\n",
      "Test pairs: 124,047\n",
      "  Positive: 24,090 (19.42%)\n",
      "  Negative: 99,957 (80.58%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train/test split...\")\n",
    "print()\n",
    "\n",
    "pairs_df['holdout'] = pairs_df.apply(\n",
    "    lambda r: (r['id_i'] in holdout_students) or (r['id_j'] in holdout_students),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "train_pairs = pairs_df[~pairs_df['holdout']].reset_index(drop=True)\n",
    "test_pairs = pairs_df[pairs_df['holdout']].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train pairs: {len(train_pairs):,}\")\n",
    "print(f\"  Positive: {train_pairs['label'].sum():,} ({train_pairs['label'].mean()*100:.2f}%)\")\n",
    "print(f\"  Negative: {(1-train_pairs['label']).sum():,} ({(1-train_pairs['label'].mean())*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "print(f\"Test pairs: {len(test_pairs):,}\")\n",
    "print(f\"  Positive: {test_pairs['label'].sum():,} ({test_pairs['label'].mean()*100:.2f}%)\")\n",
    "print(f\"  Negative: {(1-test_pairs['label']).sum():,} ({(1-test_pairs['label'].mean())*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "train_pairs = train_pairs.drop(columns=['holdout'])\n",
    "test_pairs = test_pairs.drop(columns=['holdout'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abc913-362c-4dfd-860e-e069aaac9001",
   "metadata": {},
   "source": [
    "## Label Quality Analysis\n",
    "\n",
    "Analyze the distribution and quality of generated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96ea19aa-9a5d-4a22-97b1-f31506510088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label quality analysis...\n",
      "\n",
      "Skill complementarity statistics:\n",
      "  Mean: 0.2305\n",
      "  Std: 0.1984\n",
      "\n",
      "Engagement complementarity statistics:\n",
      "  Mean: 2.4490\n",
      "  Std: 1794.5335\n",
      "\n",
      "Pairs per module:\n",
      "code_module code_presentation  n_pairs  positive_rate\n",
      "        AAA             2013J    30000       0.374500\n",
      "        AAA             2014J    30000       0.236633\n",
      "        BBB             2013B    30000       0.175000\n",
      "        BBB             2013J    30000       0.125567\n",
      "        BBB             2014B    30000       0.160700\n",
      "        BBB             2014J    30000       0.205233\n",
      "        CCC             2014B    30000       0.368100\n",
      "        CCC             2014J    30000       0.410000\n",
      "        DDD             2013B    30000       0.160133\n",
      "        DDD             2013J    30000       0.160967\n",
      "        DDD             2014B    30000       0.184367\n",
      "        DDD             2014J    30000       0.177267\n",
      "        EEE             2013J    30000       0.154167\n",
      "        EEE             2014B    30000       0.179133\n",
      "        EEE             2014J    30000       0.164700\n",
      "        FFF             2013B    30000       0.192733\n",
      "        FFF             2013J    30000       0.179500\n",
      "        FFF             2014B    30000       0.195600\n",
      "        FFF             2014J    30000       0.126467\n",
      "        GGG             2013J    30000       0.203033\n",
      "        GGG             2014B    30000       0.134700\n",
      "        GGG             2014J    30000       0.131500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Label quality analysis...\")\n",
    "print()\n",
    "\n",
    "label_analysis = {\n",
    "    'total_pairs': len(pairs_df),\n",
    "    'train_pairs': len(train_pairs),\n",
    "    'test_pairs': len(test_pairs),\n",
    "    'positive_rate_train': float(train_pairs['label'].mean()),\n",
    "    'positive_rate_test': float(test_pairs['label'].mean()),\n",
    "    'threshold': float(threshold),\n",
    "    'threshold_percentile': COMPLEMENTARITY_THRESHOLD_PERCENTILE,\n",
    "    'comp_score_stats': {\n",
    "        'mean': float(pairs_df['comp_score'].mean()),\n",
    "        'std': float(pairs_df['comp_score'].std()),\n",
    "        'min': float(pairs_df['comp_score'].min()),\n",
    "        'max': float(pairs_df['comp_score'].max()),\n",
    "        'median': float(pairs_df['comp_score'].median())\n",
    "    },\n",
    "    'skill_comp_stats': {\n",
    "        'mean': float(pairs_df['skill_comp'].mean()),\n",
    "        'std': float(pairs_df['skill_comp'].std())\n",
    "    },\n",
    "    'engage_comp_stats': {\n",
    "        'mean': float(pairs_df['engage_comp'].mean()),\n",
    "        'std': float(pairs_df['engage_comp'].std())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Skill complementarity statistics:\")\n",
    "print(f\"  Mean: {label_analysis['skill_comp_stats']['mean']:.4f}\")\n",
    "print(f\"  Std: {label_analysis['skill_comp_stats']['std']:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Engagement complementarity statistics:\")\n",
    "print(f\"  Mean: {label_analysis['engage_comp_stats']['mean']:.4f}\")\n",
    "print(f\"  Std: {label_analysis['engage_comp_stats']['std']:.4f}\")\n",
    "print()\n",
    "\n",
    "pairs_by_module = pairs_df.groupby(['code_module', 'code_presentation']).agg({\n",
    "    'label': ['count', 'mean']\n",
    "}).reset_index()\n",
    "pairs_by_module.columns = ['code_module', 'code_presentation', 'n_pairs', 'positive_rate']\n",
    "\n",
    "print(\"Pairs per module:\")\n",
    "print(pairs_by_module.to_string(index=False))\n",
    "print()\n",
    "\n",
    "label_analysis['pairs_by_module'] = pairs_by_module.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf2ad89-9163-41ed-8f03-5dab80eee684",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save train/test pairs and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f8def35-658f-41f6-bcbf-fb6940602c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving pairs...\n",
      "\n",
      "Saved files:\n",
      "  ../670-Project/data/processed/student_pairs_train.pkl\n",
      "  ../670-Project/data/processed/student_pairs_test.pkl\n",
      "  ../670-Project/data/processed/complementarity_definition.json\n",
      "  ../670-Project/results/analysis/label_distribution.pkl\n",
      "\n",
      "  student_pairs_train.pkl        (26.58 MB)\n",
      "  student_pairs_test.pkl         (6.15 MB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving pairs...\")\n",
    "print()\n",
    "\n",
    "with open(DATA_PROCESSED_DIR / 'student_pairs_train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_pairs, f)\n",
    "\n",
    "with open(DATA_PROCESSED_DIR / 'student_pairs_test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_pairs, f)\n",
    "\n",
    "with open(RESULTS_ANALYSIS_DIR / 'label_distribution.pkl', 'wb') as f:\n",
    "    pickle.dump(label_analysis, f)\n",
    "\n",
    "complementarity_definition = {\n",
    "    'skill_weight': 0.60,\n",
    "    'engagement_weight': 0.40,\n",
    "    'skill_components': {\n",
    "        'correlation': 0.40,\n",
    "        'distance': 0.35,\n",
    "        'complement_pattern': 0.25\n",
    "    },\n",
    "    'engagement_components': {\n",
    "        'type_diversity': 0.35,\n",
    "        'entropy': 0.35,\n",
    "        'activity_level': 0.30\n",
    "    },\n",
    "    'threshold_percentile': COMPLEMENTARITY_THRESHOLD_PERCENTILE,\n",
    "    'threshold_value': float(threshold),\n",
    "    'max_pairs_per_module': MAX_PAIRS_PER_MODULE,\n",
    "    'random_seed': RNG_SEED\n",
    "}\n",
    "\n",
    "with open(DATA_PROCESSED_DIR / 'complementarity_definition.json', 'w') as f:\n",
    "    json.dump(complementarity_definition, f, indent=2)\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(f\"  {DATA_PROCESSED_DIR / 'student_pairs_train.pkl'}\")\n",
    "print(f\"  {DATA_PROCESSED_DIR / 'student_pairs_test.pkl'}\")\n",
    "print(f\"  {DATA_PROCESSED_DIR / 'complementarity_definition.json'}\")\n",
    "print(f\"  {RESULTS_ANALYSIS_DIR / 'label_distribution.pkl'}\")\n",
    "print()\n",
    "\n",
    "sizes = [\n",
    "    ('student_pairs_train.pkl', DATA_PROCESSED_DIR / 'student_pairs_train.pkl'),\n",
    "    ('student_pairs_test.pkl', DATA_PROCESSED_DIR / 'student_pairs_test.pkl')\n",
    "]\n",
    "\n",
    "for name, path in sizes:\n",
    "    size_mb = path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {name:30s} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9921c6c-bfdf-4788-b79b-1eff3565ed0a",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5272d3a-d1c2-4f86-ad82-049151ef52a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL GENERATION COMPLETE\n",
      "\n",
      "Pairs created:\n",
      "  Total: 660,000\n",
      "  Train: 535,953 (81.2%)\n",
      "  Test: 124,047 (18.8%)\n",
      "\n",
      "Label distribution:\n",
      "  Positive (complementary): 132,000 (20.0%)\n",
      "  Negative: 528,000 (80.0%)\n",
      "\n",
      "Complementarity definition:\n",
      "  Skill weight: 60%\n",
      "  Engagement weight: 40%\n",
      "  Threshold: 0.8 percentile (0.3978)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LABEL GENERATION COMPLETE\")\n",
    "print()\n",
    "\n",
    "print(\"Pairs created:\")\n",
    "print(f\"  Total: {len(pairs_df):,}\")\n",
    "print(f\"  Train: {len(train_pairs):,} ({len(train_pairs)/len(pairs_df)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_pairs):,} ({len(test_pairs)/len(pairs_df)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "print(f\"  Positive (complementary): {pairs_df['label'].sum():,} ({pairs_df['label'].mean()*100:.1f}%)\")\n",
    "print(f\"  Negative: {(1-pairs_df['label']).sum():,} ({(1-pairs_df['label'].mean())*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"Complementarity definition:\")\n",
    "print(f\"  Skill weight: 60%\")\n",
    "print(f\"  Engagement weight: 40%\")\n",
    "print(f\"  Threshold: {COMPLEMENTARITY_THRESHOLD_PERCENTILE} percentile ({threshold:.4f})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ffec5-9252-4cc5-bb42-1805bb84bd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
