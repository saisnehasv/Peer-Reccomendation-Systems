{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3cc27b-d049-4f7f-8982-95dba17a3d21",
   "metadata": {},
   "source": [
    "# Peer Recommendation System\n",
    "Course: SI 670: Applied Machine Learning\n",
    "\n",
    "Name : Yuganshi Agrawal    \n",
    "uniqname: yuganshi\n",
    "\n",
    "Name : Sai Sneha Siddapura Venkataramappa  \n",
    "uniqname: saisneha\n",
    "\n",
    "### Notebook 02: Feature Engineering\n",
    "\n",
    "This notebook creates student-level features from interaction data.\n",
    "\n",
    "**Inputs:**\n",
    "- All cleaned files from `data/processed/`\n",
    "\n",
    "**Outputs:**\n",
    "- `data/features/demographic_features.pkl`\n",
    "- `data/features/assessment_features.pkl`\n",
    "- `data/features/engagement_features.pkl`\n",
    "- `data/features/resource_features.pkl`\n",
    "- `data/features/temporal_features.pkl`\n",
    "- `data/features/feature_matrix_scaled.pkl`\n",
    "- `data/features/feature_metadata.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4f3167-350f-4600-9eb2-74465bd96a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14ddb733e650>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "if 'CUDA_VISIBLE_DEVICES' not in os.environ:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad707ec-f9c8-458d-be61-b1081664de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Configuration:\n",
      "  CPUs available: 32\n",
      "  Device: cuda\n",
      "  GPUs accessible: 4\n",
      "    GPU 0: Error - CUDA call failed lazily at initialization with err\n",
      "    GPU 1: Error - CUDA call failed lazily at initialization with err\n",
      "    GPU 2: Error - CUDA call failed lazily at initialization with err\n",
      "    GPU 3: Error - CUDA call failed lazily at initialization with err\n",
      "\n",
      "Parallel Configuration:\n",
      "  Workers: 8\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Hardware detection\n",
    "N_CPU = mp.cpu_count()\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_GPU = 0\n",
    "\n",
    "print(\"Hardware Configuration:\")\n",
    "print(f\"  CPUs available: {N_CPU}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    try:\n",
    "        N_GPU = torch.cuda.device_count()\n",
    "        print(f\"  GPUs accessible: {N_GPU}\")\n",
    "        \n",
    "        for i in range(N_GPU):\n",
    "            try:\n",
    "                name = torch.cuda.get_device_name(i)\n",
    "                print(f\"    GPU {i}: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    GPU {i}: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  GPU detection failed: {e}\")\n",
    "        print(\"  Falling back to CPU\")\n",
    "        DEVICE = 'cpu'\n",
    "        N_GPU = 0\n",
    "\n",
    "print()\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    N_WORKERS = min(8, N_CPU // 2)\n",
    "else:\n",
    "    N_WORKERS = max(1, N_CPU - 2)\n",
    "\n",
    "print(f\"Parallel Configuration:\")\n",
    "print(f\"  Workers: {N_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d8835-69a5-405b-8f0b-75a831d20cf3",
   "metadata": {},
   "source": [
    "## Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7544d4c3-191e-41f2-8fe0-9d04f650fdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure:\n",
      "  Processed data: ../670-Project/data/processed\n",
      "  Features output: ../670-Project/data/features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path('../670-Project')\n",
    "DATA_PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
    "DATA_FEATURES_DIR = BASE_DIR / 'data' / 'features'\n",
    "\n",
    "DATA_FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure:\")\n",
    "print(f\"  Processed data: {DATA_PROCESSED_DIR}\")\n",
    "print(f\"  Features output: {DATA_FEATURES_DIR}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b57c26-f626-484d-861f-2e163bec0678",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e95284-fa7c-4bc8-8ac1-8567a480f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  PCA embedding dimension: 48\n",
      "  Random seed: 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EMB_DIM = 48\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  PCA embedding dimension: {EMB_DIM}\")\n",
    "print(f\"  Random seed: {RNG_SEED}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33506549-2540-4aa6-9430-34e9bd615e88",
   "metadata": {},
   "source": [
    "## Load Processed Data\n",
    "\n",
    "Load all cleaned datasets from notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da72f043-58c8-42b1-96ff-a06d26a3e8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed datasets...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e815a2a1e29143b992b794b1aac9cb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 8 datasets\n",
      "  Training students: 25,907\n",
      "  Holdout students: 2,878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading processed datasets...\")\n",
    "print()\n",
    "\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "data_files = {\n",
    "    'student_info': 'student_info_clean.pkl',\n",
    "    'student_vle': 'student_vle_clean.pkl',\n",
    "    'vle': 'vle_clean.pkl',\n",
    "    'assessments': 'assessments_clean.pkl',\n",
    "    'student_assessment': 'student_assessment_clean.pkl',\n",
    "    'courses': 'courses_clean.pkl',\n",
    "    'student_registration': 'student_registration_clean.pkl',\n",
    "    'student_split': 'student_split.pkl'\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for name, filename in tqdm(data_files.items(), desc=\"Loading files\"):\n",
    "    filepath = DATA_PROCESSED_DIR / filename\n",
    "    datasets[name] = load_pickle(filepath)\n",
    "\n",
    "student_info = datasets['student_info']\n",
    "student_vle = datasets['student_vle']\n",
    "vle = datasets['vle']\n",
    "assessments = datasets['assessments']\n",
    "student_assessment = datasets['student_assessment']\n",
    "courses = datasets['courses']\n",
    "student_registration = datasets['student_registration']\n",
    "student_split = datasets['student_split']\n",
    "\n",
    "train_students = student_split['train_students']\n",
    "holdout_students = student_split['holdout_students']\n",
    "\n",
    "print(f\"\\nLoaded {len(datasets)} datasets\")\n",
    "print(f\"  Training students: {len(train_students):,}\")\n",
    "print(f\"  Holdout students: {len(holdout_students):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174edc5d-445a-4098-84d8-63e8d68332df",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### 1. Demographic Features\n",
    "\n",
    "One-hot encode demographic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162d6e99-65f6-482a-bc0f-b8936b857afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering demographic features...\n",
      "\n",
      "Initial shape: (32593, 9)\n",
      "Unique students: 28785\n",
      "\n",
      "After one-hot encoding: (32593, 44)\n",
      "Categorical columns added: 41\n",
      "\n",
      "Sample features:\n",
      "   id_student  num_of_prev_attempts  studied_credits  gender_F  gender_M  \\\n",
      "0       11391                     0              240         0         1   \n",
      "1       28400                     0               60         1         0   \n",
      "2       30268                     0               60         1         0   \n",
      "\n",
      "   gender_nan  region_East Anglian Region  region_East Midlands Region  \\\n",
      "0           0                           1                            0   \n",
      "1           0                           0                            0   \n",
      "2           0                           0                            0   \n",
      "\n",
      "   region_Ireland  region_London Region  ...  imd_band_80-90%  \\\n",
      "0               0                     0  ...                0   \n",
      "1               0                     0  ...                0   \n",
      "2               0                     0  ...                0   \n",
      "\n",
      "   imd_band_90-100%  imd_band_nan  age_band_0-35  age_band_35-55  \\\n",
      "0                 1             0              0               0   \n",
      "1                 0             0              0               1   \n",
      "2                 0             0              0               1   \n",
      "\n",
      "   age_band_55<=  age_band_nan  disability_N  disability_Y  disability_nan  \n",
      "0              1             0             1             0               0  \n",
      "1              0             0             1             0               0  \n",
      "2              0             0             0             1               0  \n",
      "\n",
      "[3 rows x 44 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering demographic features...\")\n",
    "print()\n",
    "\n",
    "demo = student_info[[\n",
    "    'id_student', 'gender', 'region', 'highest_education', \n",
    "    'imd_band', 'age_band', 'num_of_prev_attempts', \n",
    "    'studied_credits', 'disability'\n",
    "]].copy()\n",
    "\n",
    "print(f\"Initial shape: {demo.shape}\")\n",
    "print(f\"Unique students: {demo['id_student'].nunique()}\")\n",
    "print()\n",
    "\n",
    "demo_oh = pd.get_dummies(\n",
    "    demo,\n",
    "    columns=['gender', 'region', 'highest_education', 'imd_band', 'age_band', 'disability'],\n",
    "    dummy_na=True,\n",
    "    drop_first=False\n",
    ").fillna(0)\n",
    "\n",
    "print(f\"After one-hot encoding: {demo_oh.shape}\")\n",
    "print(f\"Categorical columns added: {demo_oh.shape[1] - 3}\")\n",
    "print()\n",
    "\n",
    "demographic_features = demo_oh\n",
    "\n",
    "print(\"Sample features:\")\n",
    "print(demo_oh.head(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45c514-e778-4abc-8bbd-5f2c108aa838",
   "metadata": {},
   "source": [
    "### 2. Assessment Features\n",
    "\n",
    "Compute assessment-based features:\n",
    "- Raw scores per assessment (pivot table)\n",
    "- Score diversity (standard deviation across assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979105a1-77d9-46e1-b7c8-b4feca8f6674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering assessment features...\n",
      "\n",
      "Assessment pivot shape: (23351, 188)\n",
      "  Students: 23351\n",
      "  Assessments: 188\n",
      "\n",
      "Assessment diversity statistics:\n",
      "  Mean: 0.9131\n",
      "  Std: 0.3955\n",
      "  Range: [0.0411, 2.4994]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering assessment features...\")\n",
    "print()\n",
    "\n",
    "assess_pivot = student_assessment.pivot_table(\n",
    "    index='id_student',\n",
    "    columns='id_assessment',\n",
    "    values='score',\n",
    "    aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "print(f\"Assessment pivot shape: {assess_pivot.shape}\")\n",
    "print(f\"  Students: {len(assess_pivot)}\")\n",
    "print(f\"  Assessments: {len(assess_pivot.columns)}\")\n",
    "print()\n",
    "\n",
    "assess_pivot_z = assess_pivot.apply(\n",
    "    lambda col: (col - col.mean()) / (col.std() + 1e-8),\n",
    "    axis=0\n",
    ").fillna(0)\n",
    "\n",
    "assess_diversity = assess_pivot_z.std(axis=1).fillna(0).rename('assess_diversity')\n",
    "\n",
    "print(\"Assessment diversity statistics:\")\n",
    "print(f\"  Mean: {assess_diversity.mean():.4f}\")\n",
    "print(f\"  Std: {assess_diversity.std():.4f}\")\n",
    "print(f\"  Range: [{assess_diversity.min():.4f}, {assess_diversity.max():.4f}]\")\n",
    "print()\n",
    "\n",
    "assessment_features = {\n",
    "    'assess_pivot': assess_pivot,\n",
    "    'assess_pivot_z': assess_pivot_z,\n",
    "    'assess_diversity': assess_diversity\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9eef1-502d-46af-90c3-590256fb0fef",
   "metadata": {},
   "source": [
    "### 3. Engagement Features\n",
    "\n",
    "Compute weekly engagement patterns:\n",
    "- Weekly click aggregation\n",
    "- Mean, std, max clicks per week\n",
    "- Temporal trend (slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bf82df7-fc9f-42f8-a395-c1a84f059fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering engagement features...\n",
      "\n",
      "Weekly engagement pivot: (26074, 43)\n",
      "  Week range: -4 to 38\n",
      "\n",
      "Computing temporal trends (using parallel processing)...\n",
      "\n",
      "Engagement statistics:\n",
      "  Mean clicks/week: 35.32 (±45.02)\n",
      "  Std clicks/week: 48.52\n",
      "  Max clicks/week: 222.85\n",
      "  Trend slope: -0.7340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering engagement features...\")\n",
    "print()\n",
    "\n",
    "vle_week = student_vle.groupby(['id_student', 'week'])['sum_click'].sum().reset_index()\n",
    "\n",
    "week_pivot = vle_week.pivot(\n",
    "    index='id_student',\n",
    "    columns='week',\n",
    "    values='sum_click'\n",
    ").fillna(0)\n",
    "\n",
    "print(f\"Weekly engagement pivot: {week_pivot.shape}\")\n",
    "print(f\"  Week range: {week_pivot.columns.min()} to {week_pivot.columns.max()}\")\n",
    "print()\n",
    "\n",
    "week_mean = week_pivot.mean(axis=1).rename('week_mean')\n",
    "week_std = week_pivot.std(axis=1).fillna(0).rename('week_std')\n",
    "week_max = week_pivot.max(axis=1).rename('week_max')\n",
    "\n",
    "def compute_trend(row):\n",
    "    if len(row) > 1 and row.sum() > 0:\n",
    "        x = np.arange(len(row))\n",
    "        return np.polyfit(x, row.values, 1)[0]\n",
    "    return 0.0\n",
    "\n",
    "print(\"Computing temporal trends (using parallel processing)...\")\n",
    "\n",
    "week_trend = week_pivot.apply(compute_trend, axis=1).rename('week_trend')\n",
    "\n",
    "print()\n",
    "print(\"Engagement statistics:\")\n",
    "print(f\"  Mean clicks/week: {week_mean.mean():.2f} (±{week_mean.std():.2f})\")\n",
    "print(f\"  Std clicks/week: {week_std.mean():.2f}\")\n",
    "print(f\"  Max clicks/week: {week_max.mean():.2f}\")\n",
    "print(f\"  Trend slope: {week_trend.mean():.4f}\")\n",
    "print()\n",
    "\n",
    "engagement_features = pd.concat([\n",
    "    week_mean,\n",
    "    week_std,\n",
    "    week_max,\n",
    "    week_trend\n",
    "], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b39fe7-963d-458f-9d6c-f0581a2edb65",
   "metadata": {},
   "source": [
    "### 4. Resource Features\n",
    "\n",
    "Compute resource usage patterns:\n",
    "- Activity type diversity (number of unique types)\n",
    "- Activity type entropy (distribution evenness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f030566a-03ba-412c-a021-536452398c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering resource features...\n",
      "\n",
      "Activity type usage: 220871 student-type pairs\n",
      "\n",
      "Resource usage statistics:\n",
      "  Avg type diversity: 8.47 types per student\n",
      "  Avg entropy: 1.4668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering resource features...\")\n",
    "print()\n",
    "\n",
    "vle_types = vle[['id_site', 'activity_type']].drop_duplicates()\n",
    "svt = student_vle.merge(vle_types, on='id_site', how='left')\n",
    "\n",
    "type_counts = svt.groupby(['id_student', 'activity_type'])['sum_click'].sum().reset_index()\n",
    "\n",
    "print(f\"Activity type usage: {len(type_counts)} student-type pairs\")\n",
    "print()\n",
    "\n",
    "type_diversity = type_counts.groupby('id_student')['activity_type'].nunique().rename('type_diversity')\n",
    "\n",
    "def compute_entropy(group):\n",
    "    counts = group['sum_click'].values\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    probs = counts / total\n",
    "    return -np.sum(probs * np.log(probs + 1e-12))\n",
    "\n",
    "type_entropy = type_counts.groupby('id_student').apply(compute_entropy).rename('type_entropy')\n",
    "\n",
    "print(\"Resource usage statistics:\")\n",
    "print(f\"  Avg type diversity: {type_diversity.mean():.2f} types per student\")\n",
    "print(f\"  Avg entropy: {type_entropy.mean():.4f}\")\n",
    "print()\n",
    "\n",
    "resource_features = pd.concat([\n",
    "    type_diversity,\n",
    "    type_entropy\n",
    "], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30803955-eb14-4aa2-951b-0b90ebf40523",
   "metadata": {},
   "source": [
    "### 5. Temporal Features\n",
    "\n",
    "Registration timing features:\n",
    "- Registration date\n",
    "- Unregistration date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d9dff13-f84d-4ad7-ad23-43dbd959b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering temporal features...\n",
      "\n",
      "Registration statistics:\n",
      "  Earliest registration: -322\n",
      "  Latest registration: 101\n",
      "  Students with unregistration: 9082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering temporal features...\")\n",
    "print()\n",
    "\n",
    "reg = student_registration.groupby('id_student').agg(\n",
    "    reg_date=('date_registration', 'min'),\n",
    "    unreg_date=('date_unregistration', 'min')\n",
    ").fillna({'reg_date': 0, 'unreg_date': 9999})\n",
    "\n",
    "print(\"Registration statistics:\")\n",
    "print(f\"  Earliest registration: {reg['reg_date'].min():.0f}\")\n",
    "print(f\"  Latest registration: {reg['reg_date'].max():.0f}\")\n",
    "print(f\"  Students with unregistration: {(reg['unreg_date'] < 9999).sum()}\")\n",
    "print()\n",
    "\n",
    "temporal_features = reg.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9f365-f588-42a9-b825-209bc0e15ca7",
   "metadata": {},
   "source": [
    "## Merge All Features\n",
    "\n",
    "Combine all feature types into a single matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de1254f4-b39e-4792-a15e-a82042ed491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all features...\n",
      "\n",
      "Combined features shape: (40801, 55)\n",
      "  Students: 28785\n",
      "  Features per student: 52\n",
      "\n",
      "Feature breakdown:\n",
      "  Demographic: 43\n",
      "  Assessment: 1 (diversity)\n",
      "  Engagement: 4 (mean, std, max, trend)\n",
      "  Resource: 2 (diversity, entropy)\n",
      "  Temporal: 2 (reg_date, unreg_date)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging all features...\")\n",
    "print()\n",
    "\n",
    "features = (demographic_features\n",
    "    .merge(assess_diversity.reset_index(), on='id_student', how='left')\n",
    "    .merge(engagement_features, on='id_student', how='left')\n",
    "    .merge(resource_features, on='id_student', how='left')\n",
    "    .merge(temporal_features, on='id_student', how='left')\n",
    ").fillna(0)\n",
    "\n",
    "module_map = student_info[['id_student', 'code_module', 'code_presentation']].drop_duplicates()\n",
    "features = features.merge(module_map, on='id_student', how='left')\n",
    "\n",
    "print(f\"Combined features shape: {features.shape}\")\n",
    "print(f\"  Students: {features['id_student'].nunique()}\")\n",
    "print(f\"  Features per student: {features.shape[1] - 3}\")\n",
    "print()\n",
    "\n",
    "print(\"Feature breakdown:\")\n",
    "print(f\"  Demographic: {demographic_features.shape[1] - 1}\")\n",
    "print(f\"  Assessment: 1 (diversity)\")\n",
    "print(f\"  Engagement: 4 (mean, std, max, trend)\")\n",
    "print(f\"  Resource: 2 (diversity, entropy)\")\n",
    "print(f\"  Temporal: 2 (reg_date, unreg_date)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b79b46-e328-4670-8a9d-99043c7e2c8a",
   "metadata": {},
   "source": [
    "## Module-Wise Normalization\n",
    "\n",
    "Normalize features per module to account for module difficulty differences.\n",
    "Process in parallel batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cb78a8d-728e-49a5-9dd1-22689768ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing features per module...\n",
      "\n",
      "Processing 22 module-presentation groups...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc55533bb0648049d73fe35456baa36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Module normalization:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized features shape: (40801, 103)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalizing features per module...\")\n",
    "print()\n",
    "\n",
    "num_cols = [c for c in features.columns \n",
    "            if c not in ['id_student', 'code_module', 'code_presentation']]\n",
    "\n",
    "def normalize_module_group(args):\n",
    "    (mod, pres), group_data = args\n",
    "    \n",
    "    if len(group_data) < 2:\n",
    "        return group_data\n",
    "    \n",
    "    group = group_data.copy()\n",
    "    scaler = StandardScaler()\n",
    "    scaled_vals = scaler.fit_transform(group[num_cols].values)\n",
    "    group[num_cols] = scaled_vals\n",
    "    \n",
    "    ids = group['id_student'].values\n",
    "    assess_sub = assess_pivot_z.reindex(ids).fillna(0)\n",
    "    \n",
    "    pca_n = min(EMB_DIM, max(2, len(group) - 1), assess_sub.shape[1])\n",
    "    \n",
    "    if pca_n >= 2:\n",
    "        pca = PCA(n_components=pca_n, random_state=RNG_SEED)\n",
    "        try:\n",
    "            pca_emb = pca.fit_transform(assess_sub.values)\n",
    "        except:\n",
    "            pca_emb = np.zeros((len(group), pca_n))\n",
    "    else:\n",
    "        pca_emb = np.zeros((len(group), EMB_DIM))\n",
    "    \n",
    "    if pca_emb.shape[1] < EMB_DIM:\n",
    "        pad = np.zeros((len(group), EMB_DIM - pca_emb.shape[1]))\n",
    "        pca_emb = np.concatenate([pca_emb, pad], axis=1)\n",
    "    \n",
    "    emb_cols = [f'assess_emb_{i}' for i in range(EMB_DIM)]\n",
    "    emb_df = pd.DataFrame(pca_emb[:, :EMB_DIM], index=group.index, columns=emb_cols)\n",
    "    group = pd.concat([group.reset_index(drop=True), emb_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return group\n",
    "\n",
    "grouped = list(features.groupby(['code_module', 'code_presentation']))\n",
    "print(f\"Processing {len(grouped)} module-presentation groups...\")\n",
    "print()\n",
    "\n",
    "features_norm_list = []\n",
    "\n",
    "if N_WORKERS > 1:\n",
    "    with ProcessPoolExecutor(max_workers=N_WORKERS) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(normalize_module_group, grouped),\n",
    "            total=len(grouped),\n",
    "            desc=\"Module normalization\"\n",
    "        ))\n",
    "        features_norm_list = results\n",
    "else:\n",
    "    for group in tqdm(grouped, desc=\"Module normalization\"):\n",
    "        features_norm_list.append(normalize_module_group(group))\n",
    "\n",
    "features_proc = pd.concat(features_norm_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nNormalized features shape: {features_proc.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978577e-2d16-4d51-a910-841af37006e3",
   "metadata": {},
   "source": [
    "## Global Feature Matrix\n",
    "\n",
    "Create final feature matrix:\n",
    "1. Remove duplicate students (keep first occurrence)\n",
    "2. Global PCA for final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57c3a64c-f64c-4cd1-b09c-35b7be28c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating global feature matrix...\n",
      "\n",
      "Removed 12,016 duplicate student entries\n",
      "Unique students: 28,785\n",
      "\n",
      "Node feature matrix shape: (28785, 100)\n",
      "\n",
      "Applying global PCA...\n",
      "Global PCA explained variance: 0.9588\n",
      "\n",
      "Final embedding matrix: (28785, 49)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating global feature matrix...\")\n",
    "print()\n",
    "\n",
    "features_proc_unique = features_proc.drop_duplicates(subset=['id_student'], keep='first')\n",
    "\n",
    "print(f\"Removed {len(features_proc) - len(features_proc_unique):,} duplicate student entries\")\n",
    "print(f\"Unique students: {len(features_proc_unique):,}\")\n",
    "print()\n",
    "\n",
    "node_feature_cols = num_cols + [f'assess_emb_{i}' for i in range(EMB_DIM)]\n",
    "node_feature_matrix = features_proc_unique[['id_student'] + node_feature_cols].set_index('id_student').fillna(0)\n",
    "\n",
    "print(f\"Node feature matrix shape: {node_feature_matrix.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Applying global PCA...\")\n",
    "\n",
    "global_pca = PCA(n_components=EMB_DIM, random_state=RNG_SEED)\n",
    "node_features_global = global_pca.fit_transform(node_feature_matrix.values)\n",
    "\n",
    "print(f\"Global PCA explained variance: {global_pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print()\n",
    "\n",
    "node_feat_df = pd.DataFrame(\n",
    "    node_features_global,\n",
    "    index=node_feature_matrix.index,\n",
    "    columns=[f'emb_{i}' for i in range(EMB_DIM)]\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Final embedding matrix: {node_feat_df.shape}\")\n",
    "print()\n",
    "\n",
    "feature_matrix_scaled = node_feature_matrix.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d043c-8fed-4624-9561-501b433be6b9",
   "metadata": {},
   "source": [
    "## Save Features\n",
    "\n",
    "Save all feature sets to pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbaa0d3-8d8e-4fc6-b8aa-dce60b6494f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c99b350fa4c45548b9343eff83e8e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving features:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All features saved to: ../670-Project/data/features\n",
      "\n",
      "Saved files:\n",
      "  demographic_features.pkl                 (2.02 MB)\n",
      "  assessment_features.pkl                  (67.52 MB)\n",
      "  engagement_features.pkl                  (1.00 MB)\n",
      "  resource_features.pkl                    (0.60 MB)\n",
      "  temporal_features.pkl                    (0.66 MB)\n",
      "  feature_matrix_scaled.pkl                (22.18 MB)\n",
      "  features_proc_unique.pkl                 (22.51 MB)\n",
      "  feature_metadata.json                    (3.57 KB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving features...\")\n",
    "print()\n",
    "\n",
    "save_jobs = [\n",
    "    ('demographic_features.pkl', demographic_features),\n",
    "    ('assessment_features.pkl', assessment_features),\n",
    "    ('engagement_features.pkl', engagement_features),\n",
    "    ('resource_features.pkl', resource_features),\n",
    "    ('temporal_features.pkl', temporal_features),\n",
    "    ('feature_matrix_scaled.pkl', feature_matrix_scaled),\n",
    "    ('features_proc_unique.pkl', features_proc_unique)\n",
    "]\n",
    "\n",
    "for filename, data in tqdm(save_jobs, desc=\"Saving features\"):\n",
    "    filepath = DATA_FEATURES_DIR / filename\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "metadata = {\n",
    "    'emb_dim': EMB_DIM,\n",
    "    'num_students': len(features_proc_unique),\n",
    "    'num_features': len(node_feature_cols),\n",
    "    'pca_explained_variance': float(global_pca.explained_variance_ratio_.sum()),\n",
    "    'feature_columns': node_feature_cols,\n",
    "    'demographic_cols': [c for c in demographic_features.columns if c != 'id_student'],\n",
    "    'random_seed': RNG_SEED\n",
    "}\n",
    "\n",
    "with open(DATA_FEATURES_DIR / 'feature_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\nAll features saved to:\", DATA_FEATURES_DIR)\n",
    "print()\n",
    "\n",
    "print(\"Saved files:\")\n",
    "for filename, _ in save_jobs:\n",
    "    filepath = DATA_FEATURES_DIR / filename\n",
    "    size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {filename:40s} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"  {'feature_metadata.json':40s} ({(DATA_FEATURES_DIR / 'feature_metadata.json').stat().st_size / 1024:.2f} KB)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844aa89d-b3ce-4529-b6c5-799df9e3952b",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f508c23e-c015-4af9-be23-e8e269bf196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE ENGINEERING COMPLETE\n",
      "\n",
      "Features created:\n",
      "  Demographic: 43 features\n",
      "  Assessment: 188 raw + 1 diversity\n",
      "  Engagement: 4 temporal features\n",
      "  Resource: 2 usage features\n",
      "  Temporal: 2 registration features\n",
      "\n",
      "Final feature matrix:\n",
      "  Students: 28,785\n",
      "  Features: 100\n",
      "  PCA embedding: 48D\n",
      "  Explained variance: 0.9588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print()\n",
    "\n",
    "print(\"Features created:\")\n",
    "print(f\"  Demographic: {len([c for c in demographic_features.columns if c != 'id_student'])} features\")\n",
    "print(f\"  Assessment: {len(assess_pivot.columns)} raw + 1 diversity\")\n",
    "print(f\"  Engagement: 4 temporal features\")\n",
    "print(f\"  Resource: 2 usage features\")\n",
    "print(f\"  Temporal: 2 registration features\")\n",
    "print()\n",
    "\n",
    "print(f\"Final feature matrix:\")\n",
    "print(f\"  Students: {len(features_proc_unique):,}\")\n",
    "print(f\"  Features: {len(node_feature_cols)}\")\n",
    "print(f\"  PCA embedding: {EMB_DIM}D\")\n",
    "print(f\"  Explained variance: {global_pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
