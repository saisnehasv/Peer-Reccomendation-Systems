{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer Recommendation System - Part 3: GNN Model Training\n",
    "\n",
    "**Course**: SI 670: Applied Machine Learning\n",
    "\n",
    "**Name**: Yuganshi Agrawal  \n",
    "**uniqname**: yuganshi  \n",
    "\n",
    "\n",
    "**Name**: Sai Sneha Siddapura Venkataramappa  \n",
    "**uniqname**: saisneha\n",
    "\n",
    "This notebook trains a Graph Neural Network (GraphSAGE) model and compares it with baseline models from Notebook 2.\n",
    "\n",
    "**Workflow**:\n",
    "1. Load and process OULAD data\n",
    "2. Generate complementarity pairs\n",
    "3. Build student graph\n",
    "4. Train GraphSAGE model\n",
    "5. Load baseline models from Notebook 2 (LR, XGBoost)\n",
    "6. Compare GNN vs Baseline performance\n",
    "\n",
    "**Dependencies**: Requires Notebook 2 to be run first (generates baseline models)\n",
    "\n",
    "**Inputs**: \n",
    "- Raw OULAD CSV files from `OULAD/` directory\n",
    "- Saved baseline models from `models/checkpoints/` (from Notebook 2)\n",
    "\n",
    "**Outputs**:\n",
    "- `models/checkpoints/gnn_model.pth` - Trained GNN\n",
    "- `data/processed/gnn_embeddings.pkl` - Student embeddings\n",
    "- `results/gnn_metrics.json` - GNN performance\n",
    "- `results/gnn_vs_baselines.json` - Complete comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, itertools, sys, time\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "import xgboost as xgb\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN-BASED COMPLEMENTARY PEER RECOMMENDATION\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "RNG = 42\n",
    "np.random.seed(RNG)\n",
    "random.seed(RNG)\n",
    "torch.manual_seed(RNG)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG)\n",
    "\n",
    "DATA_DIR = 'OULAD'\n",
    "PATHS = {\n",
    "    'student_info': os.path.join(DATA_DIR, 'studentInfo.csv'),\n",
    "    'student_vle': os.path.join(DATA_DIR, 'studentVle.csv'),\n",
    "    'vle': os.path.join(DATA_DIR, 'vle.csv'),\n",
    "    'assessments': os.path.join(DATA_DIR, 'assessments.csv'),\n",
    "    'student_assessment': os.path.join(DATA_DIR, 'studentAssessment.csv'),\n",
    "    'courses': os.path.join(DATA_DIR, 'courses.csv'),\n",
    "    'student_registration': os.path.join(DATA_DIR, 'studentRegistration.csv'),\n",
    "}\n",
    "\n",
    "MAX_PAIRS_PER_MODULE = 30000\n",
    "EMB_DIM = 48\n",
    "GNN_HIDDEN = 64\n",
    "HOLDOUT_FRAC = 0.1\n",
    "BATCH_EDGE = 4096\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"GNN-BASED COMPLEMENTARY PEER RECOMMENDATION\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading data...\n",
      "Loaded 32593 students, 10655280 VLE interactions\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Loading data...\")\n",
    "for name, p in PATHS.items():\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"{p} not found\")\n",
    "\n",
    "student_info = pd.read_csv(PATHS['student_info'])\n",
    "student_vle = pd.read_csv(PATHS['student_vle'])\n",
    "vle = pd.read_csv(PATHS['vle'])\n",
    "assessments = pd.read_csv(PATHS['assessments'])\n",
    "student_assessment = pd.read_csv(PATHS['student_assessment'])\n",
    "courses = pd.read_csv(PATHS['courses'])\n",
    "student_registration = pd.read_csv(PATHS['student_registration'])\n",
    "\n",
    "if 'week' not in student_vle.columns and 'date' in student_vle.columns:\n",
    "    student_vle['week'] = (student_vle['date'] // 7).astype(int)\n",
    "\n",
    "print(f\"Loaded {len(student_info)} students, {len(student_vle)} VLE interactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Engineering features...\n",
      "Base features shape: (40801, 54)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Engineering features...\")\n",
    "\n",
    "demo = student_info[['id_student','gender','region','highest_education','imd_band',\n",
    "                     'age_band','num_of_prev_attempts','studied_credits','disability']].copy()\n",
    "demo_oh = pd.get_dummies(demo, columns=['gender','region','highest_education',\n",
    "                                         'imd_band','age_band','disability'],\n",
    "                         dummy_na=True, drop_first=False).fillna(0)\n",
    "\n",
    "assess_pivot = student_assessment.pivot_table(\n",
    "    index='id_student', columns='id_assessment',\n",
    "    values='score', aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "assess_pivot_z = assess_pivot.apply(lambda c: (c - c.mean()) / (c.std() + 1e-8), axis=0).fillna(0)\n",
    "assess_diversity = assess_pivot_z.std(axis=1).fillna(0).rename('assess_diversity')\n",
    "\n",
    "vle_week = student_vle.groupby(['id_student','week'])['sum_click'].sum().reset_index()\n",
    "week_pivot = vle_week.pivot(index='id_student', columns='week', values='sum_click').fillna(0)\n",
    "week_mean = week_pivot.mean(axis=1).rename('week_mean')\n",
    "week_std = week_pivot.std(axis=1).fillna(0).rename('week_std')\n",
    "week_trend = week_pivot.apply(lambda r: np.polyfit(range(len(r)), r.values, 1)[0] if len(r)>1 else 0, axis=1).rename('week_trend')\n",
    "\n",
    "vle_types = vle[['id_site','activity_type']].drop_duplicates()\n",
    "svt = student_vle.merge(vle_types, on='id_site', how='left')\n",
    "type_counts = svt.groupby(['id_student','activity_type'])['sum_click'].sum().reset_index()\n",
    "type_entropy = type_counts.groupby('id_student').apply(\n",
    "    lambda g: -np.sum((g['sum_click']/g['sum_click'].sum()) * np.log(g['sum_click']/g['sum_click'].sum() + 1e-12))\n",
    ").rename('type_entropy')\n",
    "type_diversity = type_counts.groupby('id_student')['activity_type'].nunique().rename('type_diversity')\n",
    "\n",
    "reg = student_registration.groupby('id_student').agg(\n",
    "    reg_date=('date_registration', 'min'),\n",
    "    unreg_date=('date_unregistration', 'min')\n",
    ").fillna({'reg_date':0, 'unreg_date':9999})\n",
    "\n",
    "features = (demo_oh\n",
    "    .merge(assess_diversity.reset_index(), on='id_student', how='left')\n",
    "    .merge(week_mean.reset_index(), on='id_student', how='left')\n",
    "    .merge(week_std.reset_index(), on='id_student', how='left')\n",
    "    .merge(week_trend.reset_index(), on='id_student', how='left')\n",
    "    .merge(type_entropy.reset_index(), on='id_student', how='left')\n",
    "    .merge(type_diversity.reset_index(), on='id_student', how='left')\n",
    "    .merge(reg.reset_index(), on='id_student', how='left')\n",
    ").fillna(0)\n",
    "\n",
    "module_map = student_info[['id_student','code_module','code_presentation']].drop_duplicates()\n",
    "features = features.merge(module_map, on='id_student', how='left')\n",
    "\n",
    "print(f\"Base features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module-wise Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Normalizing features per module...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af74d1c48bb4e93b316e7f18d6ad394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Module-normalize:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features: (28785, 49), explained variance: 0.960\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Normalizing features per module...\")\n",
    "\n",
    "num_cols = [c for c in features.columns if c not in ['id_student','code_module','code_presentation']]\n",
    "features_norm_list = []\n",
    "\n",
    "for (mod, pres), group in tqdm(features.groupby(['code_module','code_presentation']), desc=\"Module-normalize\"):\n",
    "    if len(group) < 2:\n",
    "        features_norm_list.append(group.copy())\n",
    "        continue\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_vals = scaler.fit_transform(group[num_cols].values)\n",
    "    g = group.copy()\n",
    "    g[num_cols] = scaled_vals\n",
    "\n",
    "    ids = g['id_student'].values\n",
    "    assess_sub = assess_pivot_z.reindex(ids).fillna(0)\n",
    "\n",
    "    pca_n = min(EMB_DIM, max(2, len(g)-1), assess_sub.shape[1])\n",
    "    if pca_n >= 2:\n",
    "        pca = PCA(n_components=pca_n, random_state=RNG)\n",
    "        try:\n",
    "            pca_emb = pca.fit_transform(assess_sub.values)\n",
    "        except Exception:\n",
    "            pca_emb = np.zeros((len(g), pca_n))\n",
    "    else:\n",
    "        pca_emb = np.zeros((len(g), EMB_DIM))\n",
    "\n",
    "    if pca_emb.shape[1] < EMB_DIM:\n",
    "        pad = np.zeros((len(g), EMB_DIM - pca_emb.shape[1]))\n",
    "        pca_emb = np.concatenate([pca_emb, pad], axis=1)\n",
    "\n",
    "    emb_cols = [f'assess_emb_{i}' for i in range(EMB_DIM)]\n",
    "    emb_df = pd.DataFrame(pca_emb[:, :EMB_DIM], index=g.index, columns=emb_cols)\n",
    "    g = pd.concat([g.reset_index(drop=True), emb_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    features_norm_list.append(g)\n",
    "\n",
    "features_proc = pd.concat(features_norm_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Remove duplicates for node features\n",
    "features_proc_unique = features_proc.drop_duplicates(subset=['id_student'], keep='first')\n",
    "\n",
    "node_feature_cols = num_cols + [f'assess_emb_{i}' for i in range(EMB_DIM)]\n",
    "node_feature_matrix = features_proc_unique[['id_student'] + node_feature_cols].set_index('id_student').fillna(0)\n",
    "\n",
    "global_pca = PCA(n_components=EMB_DIM, random_state=RNG)\n",
    "node_features_global = global_pca.fit_transform(node_feature_matrix.values)\n",
    "node_feat_df = pd.DataFrame(\n",
    "    node_features_global,\n",
    "    index=node_feature_matrix.index,\n",
    "    columns=[f'emb_{i}' for i in range(EMB_DIM)]\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Node features: {node_feat_df.shape}, explained variance: {global_pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "node_feat_lookup = {\n",
    "    int(r['id_student']): r[[c for c in node_feat_df.columns if c.startswith('emb_')]].values.astype(np.float32)\n",
    "    for _, r in node_feat_df.iterrows()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generating pairs with complementarity labels...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90a3a4ef824473f9ddfb5a489aaf1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pairs:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 660000 pairs, 15.00% positive\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Generating pairs with complementarity labels...\")\n",
    "\n",
    "pair_data = []\n",
    "MAX_PAIRS = MAX_PAIRS_PER_MODULE\n",
    "\n",
    "for (mod, pres), group in tqdm(features_proc.groupby(['code_module','code_presentation']), desc=\"Pairs\"):\n",
    "    # CRITICAL FIX: Remove duplicates\n",
    "    group_dedup = group.drop_duplicates(subset=['id_student'], keep='first')\n",
    "    students = group_dedup['id_student'].unique().tolist()\n",
    "    \n",
    "    if len(students) < 2:\n",
    "        continue\n",
    "\n",
    "    all_pairs = list(itertools.combinations(students, 2))\n",
    "    if len(all_pairs) > MAX_PAIRS:\n",
    "        all_pairs = random.sample(all_pairs, MAX_PAIRS)\n",
    "\n",
    "    group_indexed = group_dedup.set_index('id_student')\n",
    "    assess_cols = [c for c in group_dedup.columns if c.startswith('assess_emb_')]\n",
    "    \n",
    "    for a, b in all_pairs:\n",
    "        if a not in group_indexed.index or b not in group_indexed.index:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Skill complementarity\n",
    "            vec_a = group_indexed.loc[a, assess_cols].values.astype(float).flatten()\n",
    "            vec_b = group_indexed.loc[b, assess_cols].values.astype(float).flatten()\n",
    "            \n",
    "            if np.allclose(vec_a, 0) and np.allclose(vec_b, 0):\n",
    "                skill_comp = 0.0\n",
    "            else:\n",
    "                skill_comp = float(np.mean(np.abs(vec_a - vec_b)))\n",
    "            \n",
    "            # Engagement complementarity\n",
    "            e_a_series = group_indexed.loc[a, 'week_mean']\n",
    "            e_b_series = group_indexed.loc[b, 'week_mean']\n",
    "            \n",
    "            # Handle Series vs scalar\n",
    "            e_a = float(e_a_series.iloc[0] if hasattr(e_a_series, 'iloc') else e_a_series)\n",
    "            e_b = float(e_b_series.iloc[0] if hasattr(e_b_series, 'iloc') else e_b_series)\n",
    "            \n",
    "            engage_comp = abs(e_a - e_b)\n",
    "            overall = 0.7 * skill_comp + 0.3 * engage_comp\n",
    "            \n",
    "            pair_data.append({\n",
    "                'id_i': int(a),\n",
    "                'id_j': int(b),\n",
    "                'code_module': mod,\n",
    "                'code_presentation': pres,\n",
    "                'skill_comp': skill_comp,\n",
    "                'engage_comp': engage_comp,\n",
    "                'comp_score': overall\n",
    "            })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "pairs_df = pd.DataFrame(pair_data)\n",
    "q = pairs_df['comp_score'].quantile(0.85)\n",
    "pairs_df['label'] = (pairs_df['comp_score'] >= q).astype(int)\n",
    "\n",
    "print(f\"Created {len(pairs_df)} pairs, {pairs_df['label'].mean():.2%} positive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating train/test split...\n",
      "Train: 533625, Test: 126375\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Creating train/test split...\")\n",
    "\n",
    "all_students = features_proc_unique['id_student'].unique().tolist()\n",
    "holdout_students = set(random.sample(all_students, int(len(all_students) * HOLDOUT_FRAC)))\n",
    "pairs_df['holdout'] = pairs_df.apply(\n",
    "    lambda r: (r['id_i'] in holdout_students) or (r['id_j'] in holdout_students), axis=1\n",
    ")\n",
    "\n",
    "train_pairs = pairs_df[~pairs_df['holdout']].reset_index(drop=True)\n",
    "test_pairs = pairs_df[pairs_df['holdout']].reset_index(drop=True)\n",
    "print(f\"Train: {len(train_pairs)}, Test: {len(test_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Student Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building student graph...\n",
      "Graph: 28785 nodes, avg degree: 37.1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Building student graph...\")\n",
    "\n",
    "from collections import defaultdict\n",
    "neighbors = defaultdict(set)\n",
    "\n",
    "for _, r in train_pairs.iterrows():\n",
    "    i, j = int(r['id_i']), int(r['id_j'])\n",
    "    neighbors[i].add(j)\n",
    "    neighbors[j].add(i)\n",
    "\n",
    "for nid in node_feat_lookup.keys():\n",
    "    neighbors.setdefault(int(nid), set())\n",
    "\n",
    "node_list = sorted(list(node_feat_lookup.keys()))\n",
    "node_to_idx = {nid: idx for idx, nid in enumerate(node_list)}\n",
    "N = len(node_list)\n",
    "\n",
    "node_feats = np.zeros((N, EMB_DIM), dtype=np.float32)\n",
    "for nid, emb in node_feat_lookup.items():\n",
    "    if nid in node_to_idx:\n",
    "        node_feats[node_to_idx[nid], :] = emb\n",
    "node_feats = torch.tensor(node_feats, device=DEVICE)\n",
    "\n",
    "neighbors_idx = {node_to_idx[n]: [node_to_idx[m] for m in neighbors[n]] for n in node_list}\n",
    "\n",
    "print(f\"Graph: {N} nodes, avg degree: {np.mean([len(v) for v in neighbors_idx.values()]):.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSAGE Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.fc_self = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc_neigh = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim*2, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim*2)\n",
    "\n",
    "    def aggregate(self, node_indices, neighbors_idx):\n",
    "        neigh_feats = []\n",
    "        for n in node_indices:\n",
    "            n = int(n)\n",
    "            neigh = neighbors_idx.get(n, [])\n",
    "            if len(neigh) == 0:\n",
    "                neigh_feats.append(torch.zeros(node_feats.size(1), device=DEVICE))\n",
    "            else:\n",
    "                neigh_feats.append(node_feats[neigh].mean(dim=0))\n",
    "        return torch.stack(neigh_feats, dim=0)\n",
    "\n",
    "    def forward(self, node_indices, neighbors_idx_local):\n",
    "        x_self = node_feats[node_indices]\n",
    "        x_neigh = self.aggregate(node_indices.tolist(), neighbors_idx_local)\n",
    "\n",
    "        h_self = F.relu(self.fc_self(x_self))\n",
    "        h_neigh = F.relu(self.fc_neigh(x_neigh))\n",
    "        h = torch.cat([h_self, h_neigh], dim=1)\n",
    "        h = self.bn(h)\n",
    "        h = self.dropout(h)\n",
    "        return self.fc2(h)\n",
    "\n",
    "class EdgePredictor(nn.Module):\n",
    "    def __init__(self, node_emb_dim, hidden=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(node_emb_dim*2, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden//2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, emb_i, emb_j):\n",
    "        return self.mlp(torch.cat([emb_i, emb_j], dim=1)).squeeze(1)\n",
    "\n",
    "gnn = GraphSAGE(in_dim=EMB_DIM, hidden_dim=GNN_HIDDEN, out_dim=GNN_HIDDEN).to(DEVICE)\n",
    "edge_pred = EdgePredictor(node_emb_dim=GNN_HIDDEN, hidden=GNN_HIDDEN).to(DEVICE)\n",
    "\n",
    "opt = torch.optim.Adam(list(gnn.parameters()) + list(edge_pred.parameters()), lr=LR, weight_decay=1e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training GNN for 15 epochs...\n",
      "Epoch 1/15, Loss: 0.3218\n",
      "Epoch 2/15, Loss: 0.1773\n",
      "Epoch 3/15, Loss: 0.1591\n",
      "Epoch 4/15, Loss: 0.1510\n",
      "Epoch 5/15, Loss: 0.1464\n",
      "Epoch 6/15, Loss: 0.1416\n",
      "Epoch 7/15, Loss: 0.1388\n",
      "Epoch 8/15, Loss: 0.1355\n",
      "Epoch 9/15, Loss: 0.1339\n",
      "Epoch 10/15, Loss: 0.1312\n",
      "Epoch 11/15, Loss: 0.1299\n",
      "Epoch 12/15, Loss: 0.1288\n",
      "Epoch 13/15, Loss: 0.1279\n",
      "Epoch 14/15, Loss: 0.1265\n",
      "Epoch 15/15, Loss: 0.1252\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Training GNN for {EPOCHS} epochs...\")\n",
    "\n",
    "train_edge_idx = [\n",
    "    (node_to_idx.get(int(r['id_i']), None), node_to_idx.get(int(r['id_j']), None), r['label'])\n",
    "    for _, r in train_pairs.iterrows()\n",
    "]\n",
    "train_edge_idx = [(a, b, lbl) for (a, b, lbl) in train_edge_idx if a is not None and b is not None]\n",
    "\n",
    "def edge_batches(edge_list, batch_size=BATCH_EDGE):\n",
    "    idxs = np.random.permutation(len(edge_list))\n",
    "    for start in range(0, len(edge_list), batch_size):\n",
    "        yield [edge_list[i] for i in idxs[start:start+batch_size]]\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    gnn.train(); edge_pred.train()\n",
    "    losses = []\n",
    "    \n",
    "    for batch in edge_batches(train_edge_idx):\n",
    "        nodes_in_batch = list({a for a, b, l in batch} | {b for a,b,l in batch})\n",
    "        nodes_tensor = torch.tensor(nodes_in_batch, dtype=torch.long, device=DEVICE)\n",
    "        node_embs = gnn(nodes_tensor, neighbors_idx)\n",
    "\n",
    "        map_idx = {nid: i for i, nid in enumerate(nodes_in_batch)}\n",
    "        emb_i = torch.stack([node_embs[map_idx[a]] for a, b, l in batch])\n",
    "        emb_j = torch.stack([node_embs[map_idx[b]] for a, b, l in batch])\n",
    "        labels = torch.tensor([l for a, b, l in batch], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        logits = edge_pred(emb_i, emb_j)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(gnn.parameters()) + list(edge_pred.parameters()), 1.0)\n",
    "        opt.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}, Loss: {np.mean(losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate GNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating GNN...\n",
      "GNN RESULTS\n",
      "AUROC: 0.9764\n",
      "AUPRC: 0.8930\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97    107202\n",
      "           1       0.82      0.80      0.81     19173\n",
      "\n",
      "    accuracy                           0.94    126375\n",
      "   macro avg       0.89      0.88      0.89    126375\n",
      "weighted avg       0.94      0.94      0.94    126375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Evaluating GNN...\")\n",
    "\n",
    "gnn.eval(); edge_pred.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_node_emb = gnn(torch.arange(N, device=DEVICE), neighbors_idx)\n",
    "\n",
    "def score_pairs(pairs_df_in):\n",
    "    scores, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(pairs_df_in), BATCH_EDGE):\n",
    "            sub = pairs_df_in.iloc[start:start+BATCH_EDGE]\n",
    "            idx_pairs = []\n",
    "            for _, r in sub.iterrows():\n",
    "                a = node_to_idx.get(int(r['id_i']), None)\n",
    "                b = node_to_idx.get(int(r['id_j']), None)\n",
    "                if a is not None and b is not None:\n",
    "                    idx_pairs.append((a, b))\n",
    "                else:\n",
    "                    scores.append(0.0)\n",
    "                    labels.append(int(r['label']))\n",
    "                    continue\n",
    "            \n",
    "            if idx_pairs:\n",
    "                emb_i = all_node_emb[[a for a, b in idx_pairs]]\n",
    "                emb_j = all_node_emb[[b for a, b in idx_pairs]]\n",
    "                logits = edge_pred(emb_i, emb_j)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                scores.extend(probs.tolist())\n",
    "                labels.extend([int(r['label']) for _, r in sub.iloc[:len(idx_pairs)].iterrows()])\n",
    "    \n",
    "    return np.array(scores), np.array(labels)\n",
    "\n",
    "gnn_scores, gnn_labels = score_pairs(test_pairs)\n",
    "\n",
    "print(\"GNN RESULTS\")\n",
    "print(f\"AUROC: {roc_auc_score(gnn_labels, gnn_scores):.4f}\")\n",
    "print(f\"AUPRC: {average_precision_score(gnn_labels, gnn_scores):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(gnn_labels, (gnn_scores >= 0.5).astype(int)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Baseline Models from Notebook 2\n",
    "\n",
    "Load the pre-trained baseline models (Logistic Regression and XGBoost) that were trained in Notebook 2.\n",
    "\n",
    "**Note**: This requires Notebook 2 to have been run first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline models from Notebook 2...\n",
      "  Loaded: models/checkpoints/lr_model.pkl\n",
      "  Loaded: models/checkpoints/xgb_model.pkl\n",
      "  Loaded: results/baseline_metrics.json\n",
      "\n",
      "Baseline Model Performance (from Notebook 2):\n",
      "  Logistic Regression:\n",
      "    ROC-AUC: 0.7881852266666667\n",
      "    PR-AUC:  0.47972199260927584\n",
      "  XGBoost:\n",
      "    ROC-AUC: 0.8315369866666666\n",
      "    PR-AUC:  0.5845035938083278\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Loading baseline models from Notebook 2...\")\n",
    "\n",
    "# Check if baseline models exist\n",
    "baseline_dir = Path('models/checkpoints')\n",
    "lr_path = baseline_dir / 'lr_model.pkl'\n",
    "xgb_path = baseline_dir / 'xgb_model.pkl'\n",
    "baseline_metrics_path = Path('results/baseline_metrics.json')\n",
    "\n",
    "if not lr_path.exists() or not xgb_path.exists():\n",
    "    print(\"ERROR: Baseline models not found!\")\n",
    "    print(\"Please run Notebook 2 first to generate baseline models.\")\n",
    "    print(f\"  Expected: {lr_path}\")\n",
    "    print(f\"  Expected: {xgb_path}\")\n",
    "    baseline_metrics = None\n",
    "    lr_model = None\n",
    "    xgb_model = None\n",
    "else:\n",
    "    # Load baseline models (they're saved as dicts with 'model' and 'scaler' keys)\n",
    "    with open(lr_path, 'rb') as f:\n",
    "        lr_data = pickle.load(f)\n",
    "        lr_model = lr_data['model']\n",
    "        lr_scaler = lr_data['scaler']\n",
    "    print(f\"  Loaded: {lr_path}\")\n",
    "    \n",
    "    with open(xgb_path, 'rb') as f:\n",
    "        xgb_data = pickle.load(f)\n",
    "        xgb_model = xgb_data['model']\n",
    "        xgb_scaler = xgb_data['scaler']\n",
    "    print(f\"  Loaded: {xgb_path}\")\n",
    "    \n",
    "    # Load baseline metrics\n",
    "    if baseline_metrics_path.exists():\n",
    "        with open(baseline_metrics_path, 'r') as f:\n",
    "            baseline_metrics = json.load(f)\n",
    "        print(f\"  Loaded: {baseline_metrics_path}\")\n",
    "        \n",
    "        # Display baseline performance (note: keys have hyphens and capitals)\n",
    "        print(\"\\nBaseline Model Performance (from Notebook 2):\")\n",
    "        print(f\"  Logistic Regression:\")\n",
    "        lr_metrics = baseline_metrics.get('logistic_regression', {})\n",
    "        print(f\"    ROC-AUC: {lr_metrics.get('ROC-AUC', lr_metrics.get('roc_auc', 'N/A'))}\")\n",
    "        print(f\"    PR-AUC:  {lr_metrics.get('PR-AUC', lr_metrics.get('pr_auc', 'N/A'))}\")\n",
    "        \n",
    "        print(f\"  XGBoost:\")\n",
    "        xgb_metrics_loaded = baseline_metrics.get('xgboost', {})\n",
    "        print(f\"    ROC-AUC: {xgb_metrics_loaded.get('ROC-AUC', xgb_metrics_loaded.get('roc_auc', 'N/A'))}\")\n",
    "        print(f\"    PR-AUC:  {xgb_metrics_loaded.get('PR-AUC', xgb_metrics_loaded.get('pr_auc', 'N/A'))}\")\n",
    "    else:\n",
    "        print(f\"  Baseline metrics not found at {baseline_metrics_path}\")\n",
    "        baseline_metrics = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare GNN vs Baseline Models\n",
    "\n",
    "Compare the GNN model's performance against the baseline models from Notebook 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Model                          ROC-AUC       PR-AUC     Improvement\n",
      "----------------------------------------------------------------------\n",
      "Logistic Regression             0.7882       0.4797      (baseline)\n",
      "XGBoost                         0.8315       0.5845      (baseline)\n",
      "GraphSAGE (GNN)                 0.9764       0.8930          +17.4%\n",
      "======================================================================\n",
      "\n",
      "Best Model: GraphSAGE (ROC-AUC: 0.9764)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Comparison:\")\n",
    "\n",
    "# Get GNN metrics (make sure they're calculated)\n",
    "if 'gnn_scores' in globals() and 'gnn_labels' in globals():\n",
    "    gnn_roc_auc = roc_auc_score(gnn_labels, gnn_scores)\n",
    "    gnn_pr_auc = average_precision_score(gnn_labels, gnn_scores)\n",
    "    \n",
    "    # Display comparison table\n",
    "    if baseline_metrics:\n",
    "        print(f\"{'Model':<25} {'ROC-AUC':>12} {'PR-AUC':>12} {'Improvement':>15}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # Baseline LR (handle both key formats)\n",
    "        lr_m = baseline_metrics.get('logistic_regression', {})\n",
    "        lr_roc = lr_m.get('ROC-AUC', lr_m.get('roc_auc', 0))\n",
    "        lr_pr = lr_m.get('PR-AUC', lr_m.get('pr_auc', 0))\n",
    "        print(f\"{'Logistic Regression':<25} {lr_roc:>12.4f} {lr_pr:>12.4f} {'(baseline)':>15}\")\n",
    "        \n",
    "        # Baseline XGBoost\n",
    "        xgb_m = baseline_metrics.get('xgboost', {})\n",
    "        xgb_roc = xgb_m.get('ROC-AUC', xgb_m.get('roc_auc', 0))\n",
    "        xgb_pr = xgb_m.get('PR-AUC', xgb_m.get('pr_auc', 0))\n",
    "        print(f\"{'XGBoost':<25} {xgb_roc:>12.4f} {xgb_pr:>12.4f} {'(baseline)':>15}\")\n",
    "        \n",
    "        # GNN\n",
    "        gnn_improvement = ((gnn_roc_auc - xgb_roc) / xgb_roc * 100) if xgb_roc > 0 else 0\n",
    "        print(f\"{'GraphSAGE (GNN)':<25} {gnn_roc_auc:>12.4f} {gnn_pr_auc:>12.4f} {f'+{gnn_improvement:.1f}%':>15}\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Determine best model\n",
    "        best_model_info = max([\n",
    "            ('Logistic Regression', lr_roc),\n",
    "            ('XGBoost', xgb_roc),\n",
    "            ('GraphSAGE', gnn_roc_auc)\n",
    "        ], key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"\\nBest Model: {best_model_info[0]} (ROC-AUC: {best_model_info[1]:.4f})\")\n",
    "    else:\n",
    "        print(\"Baseline metrics not available - showing GNN results only:\")\n",
    "        print(f\"  GNN ROC-AUC: {gnn_roc_auc:.4f}\")\n",
    "        print(f\"  GNN PR-AUC:  {gnn_pr_auc:.4f}\")\n",
    "else:\n",
    "    print(\"ERROR: GNN scores not found. Make sure evaluation cell ran successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save GNN Results and Comparison\n",
    "\n",
    "Save GNN model, embeddings, and comprehensive comparison with baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving GNN model and results...\n",
      "  Saved: data/processed/gnn_embeddings.pkl\n",
      "  Saved: results/gnn_metrics.json\n",
      "  Saved: results/gnn_vs_baselines.json\n",
      "\n",
      "All outputs saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Saving GNN model and results...\")\n",
    "\n",
    "# Create directories\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "Path('models/checkpoints').mkdir(parents=True, exist_ok=True)\n",
    "Path('results').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save GNN embeddings\n",
    "if 'all_node_emb' in globals():\n",
    "    with open('data/processed/gnn_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': all_node_emb.cpu().numpy(),\n",
    "            'student_ids': node_list,\n",
    "            'node_to_idx': node_to_idx,\n",
    "            'embedding_dim': EMB_DIM\n",
    "        }, f)\n",
    "    print(\"  Saved: data/processed/gnn_embeddings.pkl\")\n",
    "\n",
    "# Save graph structure\n",
    "# if 'neighbors_idx' in globals():\n",
    "#     with open('data/processed/graph_structure.pkl', 'wb') as f:\n",
    "#         pickle.dump({\n",
    "#             'neighbors': {k: v.cpu().numpy() for k, v in neighbors_idx.items()},\n",
    "#             'node_list': node_list,\n",
    "#             'N': N\n",
    "#         }, f)\n",
    "#     print(\"  Saved: data/processed/graph_structure.pkl\")\n",
    "\n",
    "# Calculate and save GNN metrics\n",
    "if 'gnn_scores' in globals() and 'gnn_labels' in globals():\n",
    "    gnn_roc_auc = float(roc_auc_score(gnn_labels, gnn_scores))\n",
    "    gnn_pr_auc = float(average_precision_score(gnn_labels, gnn_scores))\n",
    "    \n",
    "    gnn_metrics = {\n",
    "        'model': 'GraphSAGE',\n",
    "        'ROC-AUC': gnn_roc_auc,\n",
    "        'PR-AUC': gnn_pr_auc,\n",
    "        'Accuracy': float(((gnn_scores >= 0.5) == gnn_labels).mean()),\n",
    "        'test_pairs': len(test_pairs) if 'test_pairs' in globals() else None,\n",
    "        'embedding_dim': EMB_DIM,\n",
    "        'hidden_dim': GNN_HIDDEN,\n",
    "        'device': DEVICE,\n",
    "        'type': 'classification'\n",
    "    }\n",
    "    \n",
    "    with open('results/gnn_metrics.json', 'w') as f:\n",
    "        json.dump(gnn_metrics, f, indent=2)\n",
    "    print(\"  Saved: results/gnn_metrics.json\")\n",
    "    \n",
    "    # Save comprehensive comparison\n",
    "    if baseline_metrics:\n",
    "        comparison = {\n",
    "            'baselines': baseline_metrics,\n",
    "            'gnn': gnn_metrics,\n",
    "            'improvement_over_best_baseline': {\n",
    "                'best_baseline': 'XGBoost',\n",
    "                'best_baseline_roc_auc': max(\n",
    "                    baseline_metrics.get('logistic_regression', {}).get('ROC-AUC', 0),\n",
    "                    baseline_metrics.get('xgboost', {}).get('ROC-AUC', 0)\n",
    "                ),\n",
    "                'gnn_roc_auc': gnn_roc_auc,\n",
    "                'absolute_improvement': gnn_roc_auc - max(\n",
    "                    baseline_metrics.get('logistic_regression', {}).get('ROC-AUC', 0),\n",
    "                    baseline_metrics.get('xgboost', {}).get('ROC-AUC', 0)\n",
    "                ),\n",
    "                'relative_improvement_percent': ((gnn_roc_auc - max(\n",
    "                    baseline_metrics.get('logistic_regression', {}).get('ROC-AUC', 0),\n",
    "                    baseline_metrics.get('xgboost', {}).get('ROC-AUC', 0)\n",
    "                )) / max(\n",
    "                    baseline_metrics.get('logistic_regression', {}).get('ROC-AUC', 0),\n",
    "                    baseline_metrics.get('xgboost', {}).get('ROC-AUC', 0)\n",
    "                ) * 100) if max(\n",
    "                    baseline_metrics.get('logistic_regression', {}).get('ROC-AUC', 0),\n",
    "                    baseline_metrics.get('xgboost', {}).get('ROC-AUC', 0)\n",
    "                ) > 0 else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open('results/gnn_vs_baselines.json', 'w') as f:\n",
    "            json.dump(comparison, f, indent=2)\n",
    "        print(\"  Saved: results/gnn_vs_baselines.json\")\n",
    "else:\n",
    "    print(\"  WARNING: GNN metrics not available - evaluation may not have run\")\n",
    "\n",
    "print(\"\\nAll outputs saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### GNN Model Training Complete\n",
    "\n",
    "This notebook trained a GraphSAGE model and compared it with baseline models from Notebook 2.\n",
    "\n",
    "**Approach**:\n",
    "- Used graph structure to learn student embeddings\n",
    "- 2-layer message passing with neighbor aggregation\n",
    "- Trained to predict complementarity between student pairs\n",
    "\n",
    "**Comparison**:\n",
    "- Loaded pre-trained baselines (Logistic Regression, XGBoost) from Notebook 2\n",
    "- Evaluated GNN on same test set\n",
    "- Compared performance metrics\n",
    "\n",
    "**Results**:\n",
    "- All metrics saved to `results/gnn_metrics.json`\n",
    "- Comprehensive comparison saved to `results/gnn_vs_baselines.json`\n",
    "- Student embeddings saved for use in Notebook 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
