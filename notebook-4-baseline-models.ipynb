{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a55857-07b0-4741-a83c-d07e2ca8d024",
   "metadata": {},
   "source": [
    "# Peer Recommendation System\n",
    "Course: SI 670: Applied Machine Learning\n",
    "\n",
    "Name : Yuganshi Agrawal  \n",
    "uniqname: yuganshi\n",
    "\n",
    "Name : Sai Sneha Siddapura Venkataramappa  \n",
    "uniqname: saisneha\n",
    "\n",
    "### Notebook 04: Baseline Models\n",
    "\n",
    "This notebook trains baseline models for complementarity prediction.\n",
    "\n",
    "**Inputs:**\n",
    "- `data/processed/student_pairs_train.pkl`\n",
    "- `data/processed/student_pairs_test.pkl`\n",
    "- `data/features/feature_matrix_scaled.pkl`\n",
    "\n",
    "**Outputs:**\n",
    "- `models/checkpoints/xgboost_baseline.pkl`\n",
    "- `models/checkpoints/logistic_regression_baseline.pkl`\n",
    "- `results/metrics/baseline_classification_metrics.pkl`\n",
    "- `results/metrics/baseline_ranking_metrics.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5ac7c1-bfa7-4c06-aef4-43e7797ca5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1519bcbf9cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "if 'CUDA_VISIBLE_DEVICES' not in os.environ:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, classification_report,\n",
    "    precision_recall_curve, roc_curve, confusion_matrix\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e35eed-d30e-4699-9864-fb94db1c5e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Configuration:\n",
      "  CPUs available: 32\n",
      "  Device: cpu (CUDA initialization failed: CUDA call failed lazily at initialization with error: device >= 0 && device < nu)\n",
      "\n",
      "Parallel Configuration:\n",
      "  Mode: CPU multiprocessing\n",
      "  Workers: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_CPU = mp.cpu_count()\n",
    "DEVICE = 'cpu'\n",
    "N_GPU = 0\n",
    "\n",
    "print(\"Hardware Configuration:\")\n",
    "print(f\"  CPUs available: {N_CPU}\")\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        N_GPU = torch.cuda.device_count()\n",
    "        if N_GPU > 0:\n",
    "            torch.cuda.set_device(0)\n",
    "            test_tensor = torch.zeros(1).cuda()\n",
    "            del test_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            DEVICE = 'cuda'\n",
    "            print(f\"  Device: cuda\")\n",
    "            print(f\"  GPUs accessible: {N_GPU}\")\n",
    "            \n",
    "            for i in range(N_GPU):\n",
    "                try:\n",
    "                    name = torch.cuda.get_device_name(i)\n",
    "                    mem = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "                    print(f\"    GPU {i}: {name} ({mem:.1f} GB)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    GPU {i}: Error accessing - {str(e)[:50]}\")\n",
    "        else:\n",
    "            print(f\"  Device: cpu (CUDA available but no GPUs detected)\")\n",
    "    else:\n",
    "        print(f\"  Device: cpu (CUDA not available)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  Device: cpu (CUDA initialization failed: {str(e)[:80]})\")\n",
    "    DEVICE = 'cpu'\n",
    "    N_GPU = 0\n",
    "\n",
    "print()\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    N_WORKERS = min(4, max(1, N_CPU // 4))\n",
    "else:\n",
    "    N_WORKERS = max(1, min(N_CPU - 1, 16))\n",
    "\n",
    "print(f\"Parallel Configuration:\")\n",
    "print(f\"  Mode: {'GPU-accelerated' if DEVICE == 'cuda' else 'CPU multiprocessing'}\")\n",
    "print(f\"  Workers: {N_WORKERS}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71c7dd-bb6d-48c1-862f-85633f759113",
   "metadata": {},
   "source": [
    "## Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e003ebd8-5133-462c-9028-bb5e1d96ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure:\n",
      "  Processed data: ../670-Project/data/processed\n",
      "  Features: ../670-Project/data/features\n",
      "  Models: ../670-Project/models/checkpoints\n",
      "  Metrics: ../670-Project/results/metrics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path('../670-Project')\n",
    "DATA_PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
    "DATA_FEATURES_DIR = BASE_DIR / 'data' / 'features'\n",
    "MODELS_DIR = BASE_DIR / 'models' / 'checkpoints'\n",
    "RESULTS_METRICS_DIR = BASE_DIR / 'results' / 'metrics'\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure:\")\n",
    "print(f\"  Processed data: {DATA_PROCESSED_DIR}\")\n",
    "print(f\"  Features: {DATA_FEATURES_DIR}\")\n",
    "print(f\"  Models: {MODELS_DIR}\")\n",
    "print(f\"  Metrics: {RESULTS_METRICS_DIR}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e05f7-addf-4e4d-8bf8-95d479e9591c",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0759e9de-3b16-47d0-bf0c-36afb39b3aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Batch size for feature creation: 10,000\n",
      "  Random seed: 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10000\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch size for feature creation: {BATCH_SIZE:,}\")\n",
    "print(f\"  Random seed: {RNG_SEED}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110973d2-724b-495e-8ab7-c5da6bb94059",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e1f4ac9-217f-4858-964c-f6c69e633f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Loaded data:\n",
      "  Train pairs: 535,953\n",
      "  Test pairs: 124,047\n",
      "  Feature matrix: (28785, 100)\n",
      "  Assessment pivot: (23351, 188)\n",
      "\n",
      "Label distribution:\n",
      "  Train - Positive: 107,910 (20.13%)\n",
      "  Test - Positive: 24,090 (19.42%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "print()\n",
    "\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "train_pairs = load_pickle(DATA_PROCESSED_DIR / 'student_pairs_train.pkl')\n",
    "test_pairs = load_pickle(DATA_PROCESSED_DIR / 'student_pairs_test.pkl')\n",
    "feature_matrix_scaled = load_pickle(DATA_FEATURES_DIR / 'feature_matrix_scaled.pkl')\n",
    "assessment_features = load_pickle(DATA_FEATURES_DIR / 'assessment_features.pkl')\n",
    "\n",
    "assess_pivot = assessment_features['assess_pivot']\n",
    "\n",
    "print(\"Loaded data:\")\n",
    "print(f\"  Train pairs: {len(train_pairs):,}\")\n",
    "print(f\"  Test pairs: {len(test_pairs):,}\")\n",
    "print(f\"  Feature matrix: {feature_matrix_scaled.shape}\")\n",
    "print(f\"  Assessment pivot: {assess_pivot.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "print(f\"  Train - Positive: {train_pairs['label'].sum():,} ({train_pairs['label'].mean()*100:.2f}%)\")\n",
    "print(f\"  Test - Positive: {test_pairs['label'].sum():,} ({test_pairs['label'].mean()*100:.2f}%)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75ead7-1803-4129-b749-f38006a04aad",
   "metadata": {},
   "source": [
    "## Pairwise Feature Construction\n",
    "\n",
    "Create features for each pair by combining student embeddings:\n",
    "- Absolute difference (captures dissimilarity)\n",
    "- Element-wise product (captures interaction)\n",
    "- Assessment distance (cosine-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a251b24a-ca5e-467c-88cd-d91a2c1ec705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature construction functions defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_assessment_distance(sid_i, sid_j, assess_pivot):\n",
    "    \"\"\"Compute assessment-based distance between two students.\"\"\"\n",
    "    if sid_i not in assess_pivot.index or sid_j not in assess_pivot.index:\n",
    "        return 0.5\n",
    "    \n",
    "    vec_i = assess_pivot.loc[sid_i].values\n",
    "    vec_j = assess_pivot.loc[sid_j].values\n",
    "    \n",
    "    norm_i = np.linalg.norm(vec_i)\n",
    "    norm_j = np.linalg.norm(vec_j)\n",
    "    \n",
    "    if norm_i < 1e-8 or norm_j < 1e-8:\n",
    "        return 0.5\n",
    "    \n",
    "    cos_sim = np.dot(vec_i, vec_j) / (norm_i * norm_j + 1e-8)\n",
    "    return 1 - cos_sim\n",
    "\n",
    "\n",
    "def create_pair_features_batch(pairs_batch, feature_matrix, assess_pivot):\n",
    "    \"\"\"\n",
    "    Create pairwise features for a batch of pairs.\n",
    "    \n",
    "    Returns DataFrame with features for each pair.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "    emb_cols = [c for c in feature_matrix.columns if c.startswith('emb_')]\n",
    "    n_emb = len(emb_cols)\n",
    "    \n",
    "    for _, row in pairs_batch.iterrows():\n",
    "        sid_i = row['id_i']\n",
    "        sid_j = row['id_j']\n",
    "        \n",
    "        if sid_i not in feature_matrix.index or sid_j not in feature_matrix.index:\n",
    "            feature_list.append(np.zeros(n_emb * 2 + 1))\n",
    "            continue\n",
    "        \n",
    "        emb_i = feature_matrix.loc[sid_i, emb_cols].values\n",
    "        emb_j = feature_matrix.loc[sid_j, emb_cols].values\n",
    "        \n",
    "        abs_diff = np.abs(emb_i - emb_j)\n",
    "        product = emb_i * emb_j\n",
    "        \n",
    "        assess_dist = compute_assessment_distance(sid_i, sid_j, assess_pivot)\n",
    "        \n",
    "        combined = np.concatenate([abs_diff, product, [assess_dist]])\n",
    "        feature_list.append(combined)\n",
    "    \n",
    "    feature_array = np.array(feature_list)\n",
    "    \n",
    "    col_names = (\n",
    "        [f'diff_{i}' for i in range(n_emb)] +\n",
    "        [f'prod_{i}' for i in range(n_emb)] +\n",
    "        ['assess_dist']\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame(feature_array, columns=col_names, index=pairs_batch.index)\n",
    "\n",
    "print(\"Feature construction functions defined\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c26a8b-7842-4e8d-b5eb-2eeadc873cfb",
   "metadata": {},
   "source": [
    "## Create Training Features\n",
    "\n",
    "Process pairs in batches to create pairwise features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b303f0-34be-4529-84c9-ce81d7614077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training features...\n",
      "\n",
      "Processing 535,953 pairs in 54 batches...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029c68095d3249159ae99d72f55c6167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train batches:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train features shape: (535953, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training features...\")\n",
    "print()\n",
    "\n",
    "n_batches = (len(train_pairs) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "print(f\"Processing {len(train_pairs):,} pairs in {n_batches} batches...\")\n",
    "print()\n",
    "\n",
    "train_feature_list = []\n",
    "\n",
    "for batch_idx in tqdm(range(n_batches), desc=\"Train batches\"):\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(train_pairs))\n",
    "    \n",
    "    batch_pairs = train_pairs.iloc[start_idx:end_idx]\n",
    "    batch_features = create_pair_features_batch(batch_pairs, feature_matrix_scaled, assess_pivot)\n",
    "    \n",
    "    train_feature_list.append(batch_features)\n",
    "\n",
    "train_features = pd.concat(train_feature_list, axis=0)\n",
    "train_features['label'] = train_pairs['label'].values\n",
    "\n",
    "print(f\"\\nTrain features shape: {train_features.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bfd5eb-ce62-4527-81c4-f8359f275aa1",
   "metadata": {},
   "source": [
    "## Create Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d20f2b7-8dc4-47cf-a81b-4af273fcc743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test features...\n",
      "\n",
      "Processing 124,047 pairs in 13 batches...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2418d44718d54b46a55b71513fcd9c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test features shape: (124047, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating test features...\")\n",
    "print()\n",
    "\n",
    "n_batches = (len(test_pairs) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "print(f\"Processing {len(test_pairs):,} pairs in {n_batches} batches...\")\n",
    "print()\n",
    "\n",
    "test_feature_list = []\n",
    "\n",
    "for batch_idx in tqdm(range(n_batches), desc=\"Test batches\"):\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(test_pairs))\n",
    "    \n",
    "    batch_pairs = test_pairs.iloc[start_idx:end_idx]\n",
    "    batch_features = create_pair_features_batch(batch_pairs, feature_matrix_scaled, assess_pivot)\n",
    "    \n",
    "    test_feature_list.append(batch_features)\n",
    "\n",
    "test_features = pd.concat(test_feature_list, axis=0)\n",
    "test_features['label'] = test_pairs['label'].values\n",
    "\n",
    "print(f\"\\nTest features shape: {test_features.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241d86a-1c3b-4af0-ba97-85d10196025d",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67948b2b-d84c-420c-a01a-164aa770fd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for training...\n",
      "\n",
      "X_train shape: (535953, 1)\n",
      "y_train shape: (535953,)\n",
      "X_test shape: (124047, 1)\n",
      "y_test shape: (124047,)\n",
      "\n",
      "Class distribution:\n",
      "  Train - Class 0: 428,043 (79.87%)\n",
      "  Train - Class 1: 107,910 (20.13%)\n",
      "  Test - Class 0: 99,957 (80.58%)\n",
      "  Test - Class 1: 24,090 (19.42%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data for training...\")\n",
    "print()\n",
    "\n",
    "X_train = train_features.drop(columns=['label']).values\n",
    "y_train = train_features['label'].values\n",
    "\n",
    "X_test = test_features.drop(columns=['label']).values\n",
    "y_test = test_features['label'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(f\"  Train - Class 0: {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Train - Class 1: {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  Test - Class 0: {(y_test == 0).sum():,} ({(y_test == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Test - Class 1: {(y_test == 1).sum():,} ({(y_test == 1).mean()*100:.2f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b23d3-deae-4555-82a3-2fe9bf6998d0",
   "metadata": {},
   "source": [
    "## Train Logistic Regression\n",
    "\n",
    "Train logistic regression with class balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f867c7-9d25-47dc-aeff-81d10053ea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING LOGISTIC REGRESSION\n",
      "\n",
      "Training...\n",
      "Training complete\n",
      "\n",
      "Generating predictions...\n",
      "\n",
      "Logistic Regression Results:\n",
      "  Train AUROC: 0.6329\n",
      "  Train AUPRC: 0.2950\n",
      "  Test AUROC: 0.6455\n",
      "  Test AUPRC: 0.3020\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8605    0.4498    0.5908     99957\n",
      "           1     0.2340    0.6976    0.3505     24090\n",
      "\n",
      "    accuracy                         0.4979    124047\n",
      "   macro avg     0.5473    0.5737    0.4706    124047\n",
      "weighted avg     0.7389    0.4979    0.5441    124047\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING LOGISTIC REGRESSION\")\n",
    "print()\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=RNG_SEED,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=N_WORKERS\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Training complete\")\n",
    "print()\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "lr_train_probs = lr_model.predict_proba(X_train)[:, 1]\n",
    "lr_test_probs = lr_model.predict_proba(X_test)[:, 1]\n",
    "lr_test_preds = lr_model.predict(X_test)\n",
    "print()\n",
    "\n",
    "lr_train_auroc = roc_auc_score(y_train, lr_train_probs)\n",
    "lr_train_auprc = average_precision_score(y_train, lr_train_probs)\n",
    "lr_test_auroc = roc_auc_score(y_test, lr_test_probs)\n",
    "lr_test_auprc = average_precision_score(y_test, lr_test_probs)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"  Train AUROC: {lr_train_auroc:.4f}\")\n",
    "print(f\"  Train AUPRC: {lr_train_auprc:.4f}\")\n",
    "print(f\"  Test AUROC: {lr_test_auroc:.4f}\")\n",
    "print(f\"  Test AUPRC: {lr_test_auprc:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, lr_test_preds, digits=4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca7f0f-3f40-46b8-9ad1-269d6e52fdbd",
   "metadata": {},
   "source": [
    "## Train XGBoost\n",
    "\n",
    "Train XGBoost with class balancing and hyperparameters tuned for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c20572-6d1a-4d93-a271-0a61ac39695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING XGBOOST\n",
      "\n",
      "Class imbalance ratio: 3.97\n",
      "\n",
      "Training...\n",
      "Training complete\n",
      "\n",
      "Generating predictions...\n",
      "\n",
      "XGBoost Results:\n",
      "  Train AUROC: 0.7116\n",
      "  Train AUPRC: 0.3527\n",
      "  Test AUROC: 0.7163\n",
      "  Test AUPRC: 0.3444\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8947    0.7169    0.7960     99957\n",
      "           1     0.3562    0.6499    0.4602     24090\n",
      "\n",
      "    accuracy                         0.7039    124047\n",
      "   macro avg     0.6255    0.6834    0.6281    124047\n",
      "weighted avg     0.7901    0.7039    0.7308    124047\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING XGBOOST\")\n",
    "print()\n",
    "\n",
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f\"Class imbalance ratio: {pos_weight:.2f}\")\n",
    "print()\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=pos_weight,\n",
    "    random_state=RNG_SEED,\n",
    "    tree_method='hist',\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=N_WORKERS\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "xgb_model.fit(X_train, y_train, verbose=False)\n",
    "print(\"Training complete\")\n",
    "print()\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "xgb_train_probs = xgb_model.predict_proba(X_train)[:, 1]\n",
    "xgb_test_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "xgb_test_preds = xgb_model.predict(X_test)\n",
    "print()\n",
    "\n",
    "xgb_train_auroc = roc_auc_score(y_train, xgb_train_probs)\n",
    "xgb_train_auprc = average_precision_score(y_train, xgb_train_probs)\n",
    "xgb_test_auroc = roc_auc_score(y_test, xgb_test_probs)\n",
    "xgb_test_auprc = average_precision_score(y_test, xgb_test_probs)\n",
    "\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"  Train AUROC: {xgb_train_auroc:.4f}\")\n",
    "print(f\"  Train AUPRC: {xgb_train_auprc:.4f}\")\n",
    "print(f\"  Test AUROC: {xgb_test_auroc:.4f}\")\n",
    "print(f\"  Test AUPRC: {xgb_test_auprc:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, xgb_test_preds, digits=4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44389694-4d54-48ce-b375-6d5ba6a1fd8a",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for XGBoost predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "966fd8d7-c0a6-416f-92ca-b5b25690549d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance analysis...\n",
      "\n",
      "Top 20 most important features:\n",
      "    feature  importance\n",
      "assess_dist         1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importance analysis...\")\n",
    "print()\n",
    "\n",
    "feature_names = train_features.drop(columns=['label']).columns.tolist()\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca0e7e-0928-4e8b-bbec-712deee17f79",
   "metadata": {},
   "source": [
    "## Ranking Metrics\n",
    "\n",
    "Evaluate models on ranking metrics (Recall@K, NDCG@K).\n",
    "These metrics measure how well models rank complementary students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25afd3f1-3df6-4df4-a074-1c88ac291a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking evaluation functions defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_ranking_metrics(test_pairs_df, scores, k_values=[1, 5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Compute ranking metrics for recommendation task.\n",
    "    \n",
    "    For each query student, rank candidates by score and compute:\n",
    "    - Recall@K: Fraction of true complements in top-K\n",
    "    - NDCG@K: Normalized Discounted Cumulative Gain\n",
    "    \"\"\"\n",
    "    results = {k: {'recall': [], 'ndcg': []} for k in k_values}\n",
    "    \n",
    "    test_pairs_with_scores = test_pairs_df.copy()\n",
    "    test_pairs_with_scores['score'] = scores\n",
    "    \n",
    "    query_groups = test_pairs_with_scores[test_pairs_with_scores['label'] == 1].groupby('id_i')\n",
    "    \n",
    "    eval_count = 0\n",
    "    \n",
    "    for query_sid, group in tqdm(query_groups, desc=\"Computing ranking metrics\"):\n",
    "        true_complements = set(group['id_j'].tolist())\n",
    "        \n",
    "        query_candidates = test_pairs_with_scores[\n",
    "            test_pairs_with_scores['id_i'] == query_sid\n",
    "        ].copy()\n",
    "        \n",
    "        if len(query_candidates) < 2:\n",
    "            continue\n",
    "        \n",
    "        query_candidates = query_candidates.sort_values('score', ascending=False)\n",
    "        ranked_ids = query_candidates['id_j'].tolist()\n",
    "        \n",
    "        for k in k_values:\n",
    "            if k > len(ranked_ids):\n",
    "                continue\n",
    "            \n",
    "            top_k = set(ranked_ids[:k])\n",
    "            \n",
    "            recall = len(top_k & true_complements) / len(true_complements) if true_complements else 0\n",
    "            results[k]['recall'].append(recall)\n",
    "            \n",
    "            relevance = [1 if sid in true_complements else 0 for sid in ranked_ids[:k]]\n",
    "            dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance)])\n",
    "            idcg = sum([1 / np.log2(i + 2) for i in range(min(k, len(true_complements)))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            results[k]['ndcg'].append(ndcg)\n",
    "        \n",
    "        eval_count += 1\n",
    "    \n",
    "    print(f\"  Evaluated {eval_count} queries\")\n",
    "    print()\n",
    "    \n",
    "    for k in k_values:\n",
    "        if len(results[k]['recall']) > 0:\n",
    "            print(f\"  Recall@{k}: {np.mean(results[k]['recall']):.4f} (±{np.std(results[k]['recall']):.4f})\")\n",
    "            print(f\"  NDCG@{k}: {np.mean(results[k]['ndcg']):.4f} (±{np.std(results[k]['ndcg']):.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Ranking evaluation functions defined\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1989e9-117f-44c4-9def-76ac0b621c7e",
   "metadata": {},
   "source": [
    "### Logistic Regression Ranking Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c25db993-9fbe-487d-8586-c4865c4b12c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION RANKING METRICS\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e30702953e44c2a8bcbd200b184d8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing ranking metrics:   0%|          | 0/9528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluated 8227 queries\n",
      "\n",
      "  Recall@1: 0.3371 (±0.4016)\n",
      "  NDCG@1: 0.5332 (±0.4989)\n",
      "  Recall@5: 0.6470 (±0.3817)\n",
      "  NDCG@5: 0.5686 (±0.3246)\n",
      "  Recall@10: 0.6574 (±0.3407)\n",
      "  NDCG@10: 0.5281 (±0.2864)\n",
      "  Recall@20: 0.7832 (±0.2886)\n",
      "  NDCG@20: 0.5500 (±0.2469)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LOGISTIC REGRESSION RANKING METRICS\")\n",
    "print()\n",
    "\n",
    "lr_ranking_results = compute_ranking_metrics(test_pairs, lr_test_probs)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947357ec-5b2c-4448-ba77-0962eec32130",
   "metadata": {},
   "source": [
    "### XGBoost Ranking Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ec77948-c232-4d48-a18c-a51d85ac8c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOOST RANKING METRICS\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41a40158a8941248483b4b06d18876b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing ranking metrics:   0%|          | 0/9528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluated 8227 queries\n",
      "\n",
      "  Recall@1: 0.3453 (±0.4014)\n",
      "  NDCG@1: 0.5517 (±0.4973)\n",
      "  Recall@5: 0.6603 (±0.3745)\n",
      "  NDCG@5: 0.5882 (±0.3261)\n",
      "  Recall@10: 0.6833 (±0.3293)\n",
      "  NDCG@10: 0.5556 (±0.2875)\n",
      "  Recall@20: 0.7887 (±0.2776)\n",
      "  NDCG@20: 0.5701 (±0.2536)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBOOST RANKING METRICS\")\n",
    "print()\n",
    "\n",
    "xgb_ranking_results = compute_ranking_metrics(test_pairs, xgb_test_probs)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5b5aa-28c5-44f3-949d-9bbe6d610dab",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b124ad8-dd9e-4116-970a-a9433ceedbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix analysis...\n",
      "\n",
      "Logistic Regression Confusion Matrix:\n",
      "  TN: 44,961  FP: 54,996\n",
      "  FN: 7,286  TP: 16,804\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "  TN: 71,660  FP: 28,297\n",
      "  FN: 8,433  TP: 15,657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix analysis...\")\n",
    "print()\n",
    "\n",
    "lr_cm = confusion_matrix(y_test, lr_test_preds)\n",
    "xgb_cm = confusion_matrix(y_test, xgb_test_preds)\n",
    "\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(f\"  TN: {lr_cm[0,0]:,}  FP: {lr_cm[0,1]:,}\")\n",
    "print(f\"  FN: {lr_cm[1,0]:,}  TP: {lr_cm[1,1]:,}\")\n",
    "print()\n",
    "\n",
    "print(\"XGBoost Confusion Matrix:\")\n",
    "print(f\"  TN: {xgb_cm[0,0]:,}  FP: {xgb_cm[0,1]:,}\")\n",
    "print(f\"  FN: {xgb_cm[1,0]:,}  TP: {xgb_cm[1,1]:,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4766b-10b5-437a-bf32-5c38fc4d3a44",
   "metadata": {},
   "source": [
    "## Save Models and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "546a8e5d-3784-4fc5-a1de-6b88e9c3e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models...\n",
      "\n",
      "Models saved:\n",
      "  ../670-Project/models/checkpoints/logistic_regression_baseline.pkl\n",
      "  ../670-Project/models/checkpoints/xgboost_baseline.pkl\n",
      "\n",
      "Saving metrics...\n",
      "\n",
      "Metrics saved:\n",
      "  ../670-Project/results/metrics/baseline_classification_metrics.pkl\n",
      "  ../670-Project/results/metrics/baseline_ranking_metrics.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving models...\")\n",
    "print()\n",
    "\n",
    "with open(MODELS_DIR / 'logistic_regression_baseline.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "with open(MODELS_DIR / 'xgboost_baseline.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\n",
    "print(\"Models saved:\")\n",
    "print(f\"  {MODELS_DIR / 'logistic_regression_baseline.pkl'}\")\n",
    "print(f\"  {MODELS_DIR / 'xgboost_baseline.pkl'}\")\n",
    "print()\n",
    "\n",
    "# Cell 18: Save metrics\n",
    "print(\"Saving metrics...\")\n",
    "print()\n",
    "\n",
    "classification_metrics = {\n",
    "    'logistic_regression': {\n",
    "        'train_auroc': float(lr_train_auroc),\n",
    "        'train_auprc': float(lr_train_auprc),\n",
    "        'test_auroc': float(lr_test_auroc),\n",
    "        'test_auprc': float(lr_test_auprc),\n",
    "        'confusion_matrix': lr_cm.tolist(),\n",
    "        'test_predictions': lr_test_preds.tolist(),\n",
    "        'test_probabilities': lr_test_probs.tolist()\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'train_auroc': float(xgb_train_auroc),\n",
    "        'train_auprc': float(xgb_train_auprc),\n",
    "        'test_auroc': float(xgb_test_auroc),\n",
    "        'test_auprc': float(xgb_test_auprc),\n",
    "        'confusion_matrix': xgb_cm.tolist(),\n",
    "        'test_predictions': xgb_test_preds.tolist(),\n",
    "        'test_probabilities': xgb_test_probs.tolist(),\n",
    "        'feature_importance': importance_df.to_dict('records')\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(RESULTS_METRICS_DIR / 'baseline_classification_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(classification_metrics, f)\n",
    "\n",
    "ranking_metrics = {\n",
    "    'logistic_regression': {\n",
    "        k: {\n",
    "            'recall_mean': float(np.mean(lr_ranking_results[k]['recall'])) if lr_ranking_results[k]['recall'] else 0,\n",
    "            'recall_std': float(np.std(lr_ranking_results[k]['recall'])) if lr_ranking_results[k]['recall'] else 0,\n",
    "            'ndcg_mean': float(np.mean(lr_ranking_results[k]['ndcg'])) if lr_ranking_results[k]['ndcg'] else 0,\n",
    "            'ndcg_std': float(np.std(lr_ranking_results[k]['ndcg'])) if lr_ranking_results[k]['ndcg'] else 0\n",
    "        }\n",
    "        for k in [1, 5, 10, 20]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        k: {\n",
    "            'recall_mean': float(np.mean(xgb_ranking_results[k]['recall'])) if xgb_ranking_results[k]['recall'] else 0,\n",
    "            'recall_std': float(np.std(xgb_ranking_results[k]['recall'])) if xgb_ranking_results[k]['recall'] else 0,\n",
    "            'ndcg_mean': float(np.mean(xgb_ranking_results[k]['ndcg'])) if xgb_ranking_results[k]['ndcg'] else 0,\n",
    "            'ndcg_std': float(np.std(xgb_ranking_results[k]['ndcg'])) if xgb_ranking_results[k]['ndcg'] else 0\n",
    "        }\n",
    "        for k in [1, 5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(RESULTS_METRICS_DIR / 'baseline_ranking_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(ranking_metrics, f)\n",
    "\n",
    "print(\"Metrics saved:\")\n",
    "print(f\"  {RESULTS_METRICS_DIR / 'baseline_classification_metrics.pkl'}\")\n",
    "print(f\"  {RESULTS_METRICS_DIR / 'baseline_ranking_metrics.pkl'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea8b12-4667-44e1-8b0e-231515508e04",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8b182db-df1c-422f-9400-7dcfe0b8e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE MODELS COMPLETE\n",
      "\n",
      "Models trained:\n",
      "  1. Logistic Regression\n",
      "  2. XGBoost\n",
      "\n",
      "Classification Performance (Test Set):\n",
      "  Logistic Regression - AUROC: 0.6455, AUPRC: 0.3020\n",
      "  XGBoost            - AUROC: 0.7163, AUPRC: 0.3444\n",
      "\n",
      "Ranking Performance (Test Set):\n",
      "  Logistic Regression - Recall@10: 0.6574\n",
      "  XGBoost            - Recall@10: 0.6833\n",
      "\n",
      "Best baseline model: XGBoost\n",
      "  Improvement over LR: +10.96%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"BASELINE MODELS COMPLETE\")\n",
    "print()\n",
    "\n",
    "print(\"Models trained:\")\n",
    "print(\"  1. Logistic Regression\")\n",
    "print(\"  2. XGBoost\")\n",
    "print()\n",
    "\n",
    "print(\"Classification Performance (Test Set):\")\n",
    "print(f\"  Logistic Regression - AUROC: {lr_test_auroc:.4f}, AUPRC: {lr_test_auprc:.4f}\")\n",
    "print(f\"  XGBoost            - AUROC: {xgb_test_auroc:.4f}, AUPRC: {xgb_test_auprc:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Ranking Performance (Test Set):\")\n",
    "print(f\"  Logistic Regression - Recall@10: {np.mean(lr_ranking_results[10]['recall']):.4f}\")\n",
    "print(f\"  XGBoost            - Recall@10: {np.mean(xgb_ranking_results[10]['recall']):.4f}\")\n",
    "print()\n",
    "\n",
    "if xgb_test_auroc > lr_test_auroc:\n",
    "    print(\"Best baseline model: XGBoost\")\n",
    "    improvement = (xgb_test_auroc - lr_test_auroc) / lr_test_auroc * 100\n",
    "    print(f\"  Improvement over LR: +{improvement:.2f}%\")\n",
    "else:\n",
    "    print(\"Best baseline model: Logistic Regression\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2185b4-9c5c-4050-84b8-10247c43a12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
